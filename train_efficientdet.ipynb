{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stuff\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from itertools import product as product\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function\n",
    "from utils.to_fp16 import network_to_half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset\n",
    "from utils.dataset import VOCDataset, DatasetTransform, make_datapath_list, Anno_xml2list, od_collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## meta settings\n",
    "\n",
    "# select from efficientnet backbone or resnet backbone\n",
    "backbone = \"efficientnet-b0\"\n",
    "scale = 2\n",
    "# scale==1: resolution 300\n",
    "# scale==2: resolution 600\n",
    "useBiFPN = True\n",
    "HALF = False # enable FP16\n",
    "DATASET = \"VOC\"\n",
    "retina = False # for trying retinanets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make data.Dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainlist:  16551\n",
      "vallist:  4952\n"
     ]
    }
   ],
   "source": [
    "if not DATASET == \"COCO\":\n",
    "    # load files\n",
    "    # set your VOCdevkit path here.\n",
    "    vocpath = \"../VOCdevkit/VOC2007\"\n",
    "    train_img_list, train_anno_list, val_img_list, val_anno_list = make_datapath_list(vocpath)\n",
    "\n",
    "    vocpath = \"../VOCdevkit/VOC2012\"\n",
    "    train_img_list2, train_anno_list2, _, _ = make_datapath_list(vocpath)\n",
    "\n",
    "    train_img_list.extend(train_img_list2)\n",
    "    train_anno_list.extend(train_anno_list2)\n",
    "\n",
    "    print(\"trainlist: \", len(train_img_list))\n",
    "    print(\"vallist: \", len(val_img_list))\n",
    "\n",
    "    # make Dataset\n",
    "    voc_classes = ['aeroplane', 'bicycle', 'bird', 'boat',\n",
    "                   'bottle', 'bus', 'car', 'cat', 'chair',\n",
    "                   'cow', 'diningtable', 'dog', 'horse',\n",
    "                   'motorbike', 'person', 'pottedplant',\n",
    "                   'sheep', 'sofa', 'train', 'tvmonitor']\n",
    "\n",
    "    color_mean = (104, 117, 123)  # (BGR)の色の平均値\n",
    "    if scale == 1:\n",
    "        input_size = 300  # 画像のinputサイズを300×300にする\n",
    "    else:\n",
    "        input_size = 512\n",
    "\n",
    "    ## DatasetTransformを適応\n",
    "    transform = DatasetTransform(input_size, color_mean)\n",
    "    transform_anno = Anno_xml2list(voc_classes)\n",
    "\n",
    "    # Dataloaderに入れるデータセットファイル。\n",
    "    # ゲットで叩くと画像とGTを前処理して出力してくれる。\n",
    "    train_dataset = VOCDataset(train_img_list, train_anno_list, phase = \"train\", transform=transform, transform_anno = transform_anno)\n",
    "    val_dataset = VOCDataset(val_img_list, val_anno_list, phase=\"val\", transform=DatasetTransform(\n",
    "        input_size, color_mean), transform_anno=Anno_xml2list(voc_classes))\n",
    "\n",
    "else:\n",
    "    from dataset.coco import COCODetection\n",
    "    import torch.utils.data as data\n",
    "    from utils.dataset import VOCDataset, COCODatasetTransform, make_datapath_list, Anno_xml2list, od_collate_fn\n",
    "\n",
    "    color_mean = (104, 117, 123)  # (BGR)の色の平均値\n",
    "    if scale == 1:\n",
    "        input_size = 300  # 画像のinputサイズを300×300にする\n",
    "    else:\n",
    "        input_size = 512\n",
    "\n",
    "    ## DatasetTransformを適応\n",
    "    transform = COCODatasetTransform(input_size, color_mean)\n",
    "    train_dataset = COCODetection(\"../data/coco/\", image_set=\"train2014\", phase=\"train\", transform=transform)\n",
    "    val_dataset = COCODetection(\"../data/coco/\", image_set=\"val2014\", phase=\"val\", transform=transform)\n",
    "\n",
    "batch_size = int(32/scale)\n",
    "\n",
    "train_dataloader = data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, collate_fn=od_collate_fn, num_workers=8)\n",
    "\n",
    "val_dataloader = data.DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False, collate_fn=od_collate_fn, num_workers=8)\n",
    "\n",
    "# 辞書型変数にまとめる\n",
    "dataloaders_dict = {\"train\": train_dataloader, \"val\": val_dataloader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 512, 512])\n",
      "16\n",
      "torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "# 動作の確認\n",
    "batch_iterator = iter(dataloaders_dict[\"val\"])  # イタレータに変換\n",
    "images, targets = next(batch_iterator)  # 1番目の要素を取り出す\n",
    "print(images.size())  # torch.Size([4, 3, 300, 300])\n",
    "print(len(targets))\n",
    "print(targets[1].shape)  # ミニバッチのサイズのリスト、各要素は[n, 5]、nは物体数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define EfficientDet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.efficientdet import EfficientDet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n",
      "EfficientNet(\n",
      "  (_conv_stem): Conv2dStaticSamePadding(\n",
      "    3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False\n",
      "    (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
      "  )\n",
      "  (_bn0): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "  (_blocks): ModuleList(\n",
      "    (0): MBConvBlock(\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        32, 32, kernel_size=(3, 3), stride=[1, 1], groups=32, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        32, 8, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        8, 32, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(16, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (1): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        96, 96, kernel_size=(3, 3), stride=[2, 2], groups=96, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        96, 4, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        4, 96, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (2): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        144, 144, kernel_size=(3, 3), stride=(1, 1), groups=144, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (3): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        144, 144, kernel_size=(5, 5), stride=[2, 2], groups=144, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(40, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (4): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        240, 240, kernel_size=(5, 5), stride=(1, 1), groups=240, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(40, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (5): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        240, 240, kernel_size=(3, 3), stride=[2, 2], groups=240, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (6): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (7): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (8): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        480, 480, kernel_size=(5, 5), stride=[1, 1], groups=480, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (9): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (10): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (11): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        672, 672, kernel_size=(5, 5), stride=[2, 2], groups=672, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (12): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (13): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (14): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (15): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        1152, 1152, kernel_size=(3, 3), stride=[1, 1], groups=1152, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(320, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "  )\n",
      "  (_conv_head): Conv2dStaticSamePadding(\n",
      "    320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "    (static_padding): Identity()\n",
      "  )\n",
      "  (_bn1): BatchNorm2d(1280, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "  (_avg_pooling): AdaptiveAvgPool2d(output_size=1)\n",
      "  (_dropout): Dropout(p=0.2)\n",
      "  (_fc): Linear(in_features=1280, out_features=1000, bias=True)\n",
      "  (_swish): MemoryEfficientSwish()\n",
      ")\n",
      "320\n",
      "use BiFPN\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layerc3: torch.Size([1, 40, 64, 64])\n",
      "layerc4: torch.Size([1, 80, 32, 32])\n",
      "layerc5: torch.Size([1, 320, 16, 16])\n",
      "layer size: torch.Size([1, 256, 64, 64])\n",
      "layer size: torch.Size([1, 256, 32, 32])\n",
      "layer size: torch.Size([1, 256, 16, 16])\n",
      "layer size: torch.Size([1, 256, 8, 8])\n",
      "layer size: torch.Size([1, 256, 4, 4])\n",
      "layer size: torch.Size([1, 256, 2, 2])\n",
      "torch.Size([1, 24528, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/functional.py:2457: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/functional.py:2539: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    }
   ],
   "source": [
    "if not DATASET == \"COCO\":\n",
    "    num_class = 21\n",
    "else:\n",
    "    num_class = 81\n",
    "\n",
    "if scale==1:\n",
    "    ssd_cfg = {\n",
    "        'num_classes': num_class,  # 背景クラスを含めた合計クラス数\n",
    "        'input_size': 300*scale,  # 画像の入力サイズ\n",
    "        'bbox_aspect_num': [4, 6, 6, 6, 4, 4],  # 出力するDBoxのアスペクト比の種類\n",
    "        'feature_maps': [37, 18, 9, 5, 3, 1],  # 各sourceの画像サイズ\n",
    "        'steps': [8, 16, 32, 64, 100, 300],  # DBOXの大きさを決める\n",
    "        'min_sizes': [30, 60, 111, 162, 213, 264],  # DBOXの大きさを決める\n",
    "        'max_sizes': [60, 111, 162, 213, 264, 315],  # DBOXの大きさを決める\n",
    "        'aspect_ratios': [[2], [2, 3], [2, 3], [2, 3], [2], [2]],\n",
    "    }\n",
    "elif scale==2:\n",
    "    ssd_cfg = {\n",
    "        'num_classes': num_class,  # 背景クラスを含めた合計クラス数\n",
    "        'input_size': 512,  # 画像の入力サイズ\n",
    "        'bbox_aspect_num': [4, 6, 6, 6, 4, 4],  # 出力するDBoxのアスペクト比の種類\n",
    "        'feature_maps': [64, 32, 16, 8, 4, 2],  # 各sourceの画像サイズ\n",
    "        'steps': [8, 16, 32, 64, 100, 300],  # DBOXの大きさを決める\n",
    "        'min_sizes': [30, 60, 111, 162, 213, 264]*scale,  # DBOXの大きさを決める\n",
    "        'max_sizes': [60, 111, 162, 213, 264, 315]*scale,  # DBOXの大きさを決める\n",
    "        'aspect_ratios': [[2], [2, 3], [2, 3], [2, 3], [2], [2]],\n",
    "    }\n",
    "\n",
    "# test if net works\n",
    "net = EfficientDet(phase=\"train\", cfg=ssd_cfg, verbose=True, backbone=backbone, useBiFPN=useBiFPN)\n",
    "out = net(torch.rand([1,3,input_size,input_size]))\n",
    "print(out[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n",
      "EfficientNet(\n",
      "  (_conv_stem): Conv2dStaticSamePadding(\n",
      "    3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False\n",
      "    (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
      "  )\n",
      "  (_bn0): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "  (_blocks): ModuleList(\n",
      "    (0): MBConvBlock(\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        32, 32, kernel_size=(3, 3), stride=[1, 1], groups=32, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        32, 8, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        8, 32, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(16, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (1): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        96, 96, kernel_size=(3, 3), stride=[2, 2], groups=96, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        96, 4, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        4, 96, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (2): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        144, 144, kernel_size=(3, 3), stride=(1, 1), groups=144, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (3): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        144, 144, kernel_size=(5, 5), stride=[2, 2], groups=144, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(40, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (4): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        240, 240, kernel_size=(5, 5), stride=(1, 1), groups=240, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(40, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (5): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        240, 240, kernel_size=(3, 3), stride=[2, 2], groups=240, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (6): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (7): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (8): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        480, 480, kernel_size=(5, 5), stride=[1, 1], groups=480, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (9): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (10): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (11): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        672, 672, kernel_size=(5, 5), stride=[2, 2], groups=672, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (12): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (13): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (14): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (15): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        1152, 1152, kernel_size=(3, 3), stride=[1, 1], groups=1152, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(320, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "  )\n",
      "  (_conv_head): Conv2dStaticSamePadding(\n",
      "    320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "    (static_padding): Identity()\n",
      "  )\n",
      "  (_bn1): BatchNorm2d(1280, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "  (_avg_pooling): AdaptiveAvgPool2d(output_size=1)\n",
      "  (_dropout): Dropout(p=0.2)\n",
      "  (_fc): Linear(in_features=1280, out_features=1000, bias=True)\n",
      "  (_swish): MemoryEfficientSwish()\n",
      ")\n",
      "320\n",
      "use BiFPN\n",
      "using: cuda:0\n",
      "set weights!\n"
     ]
    }
   ],
   "source": [
    "net = EfficientDet(phase=\"train\", cfg=ssd_cfg, verbose=False, backbone=backbone, useBiFPN=useBiFPN)\n",
    "\n",
    "# call retinanet for test purpose\n",
    "if retina:\n",
    "    from utils.retinanet import RetinaFPN\n",
    "    ssd_cfg = {\n",
    "        'num_classes': num_class,  # 背景クラスを含めた合計クラス数\n",
    "        'input_size': 300*scale,  # 画像の入力サイズ\n",
    "        'bbox_aspect_num': [4, 6, 6, 6, 4, 4],  # 出力するDBoxのアスペクト比の種類\n",
    "        'feature_maps': [38, 19, 10, 5, 3, 1],  # 各sourceの画像サイズ\n",
    "        'steps': [8, 16, 32, 64, 100, 300],  # DBOXの大きさを決める\n",
    "        'min_sizes': [30, 60, 111, 162, 213, 264],  # DBOXの大きさを決める\n",
    "        'max_sizes': [60, 111, 162, 213, 264, 315],  # DBOXの大きさを決める\n",
    "        'aspect_ratios': [[2], [2, 3], [2, 3], [2, 3], [2], [2]],\n",
    "    }\n",
    "    net = RetinaFPN(\"train\", ssd_cfg)\n",
    "\n",
    "# GPUが使えるか確認\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"using:\", device)\n",
    "\n",
    "print(\"set weights!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FP16..\n",
    "if HALF:\n",
    "    net = network_to_half(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze backbone layers\n",
    "for param in net.layer0.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in net.layer2.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in net.layer3.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in net.layer4.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in net.layer5.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ssd_model import MultiBoxLoss\n",
    "# define loss\n",
    "criterion = MultiBoxLoss(jaccard_thresh=0.5,neg_pos=3, device=device, half=HALF)\n",
    "# optim\n",
    "import torch.optim as optim\n",
    "optimizer = optim.SGD(filter(lambda p: p.requires_grad, net.parameters()), lr=1e-3, momentum=0.9, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# while the original efficientdet uses cosine annealining lr scheduling, we utilize epoch-based lr decreasing for simplicity.\n",
    "def get_current_lr(epoch): \n",
    "    if DATASET == \"COCO\":\n",
    "        reduce = [120, 180]\n",
    "        lr = 1e-3\n",
    "    else:\n",
    "        reduce = [120,180]\n",
    "        lr = 1e-3\n",
    "        \n",
    "    for i,lr_decay_epoch in enumerate(reduce):\n",
    "        if epoch >= lr_decay_epoch:\n",
    "            lr *= 0.1\n",
    "    return lr\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    lr = get_current_lr(epoch)\n",
    "    print(\"lr is:\", lr)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train script. nothing special..\n",
    "def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs):\n",
    "\n",
    "    # GPUが使えるかを確認\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"used device:\", device)\n",
    "\n",
    "    # ネットワークをGPUへ\n",
    "    net.to(device)\n",
    "\n",
    "    # ネットワークがある程度固定であれば、高速化させる\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    # イテレーションカウンタをセット\n",
    "    iteration = 1\n",
    "    epoch_train_loss = 0.0  # epochの損失和\n",
    "    epoch_val_loss = 0.0  # epochの損失和\n",
    "    logs = []\n",
    "\n",
    "    # epochのループ\n",
    "    for epoch in range(num_epochs+1):\n",
    "        \n",
    "        adjust_learning_rate(optimizer, epoch)\n",
    "        \n",
    "        # 開始時刻を保存\n",
    "        t_epoch_start = time.time()\n",
    "        t_iter_start = time.time()\n",
    "\n",
    "        print('-------------')\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        print('-------------')\n",
    "\n",
    "        # epochごとの訓練と検証のループ\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                net.train()  # モデルを訓練モードに\n",
    "                print('(train)')\n",
    "            else:\n",
    "                if((epoch+1) % 10 == 0):\n",
    "                    net.eval()   # モデルを検証モードに\n",
    "                    print('-------------')\n",
    "                    print('(val)')\n",
    "                else:\n",
    "                    # 検証は10回に1回だけ行う\n",
    "                    continue\n",
    "\n",
    "            # データローダーからminibatchずつ取り出すループ\n",
    "            for images, targets in dataloaders_dict[phase]:\n",
    "\n",
    "                # GPUが使えるならGPUにデータを送る\n",
    "                images = images.to(device)\n",
    "                targets = [ann.to(device)\n",
    "                           for ann in targets]  # リストの各要素のテンソルをGPUへ\n",
    "                if HALF:\n",
    "                    images = images.half()\n",
    "                    targets = [ann.half() for ann in targets]\n",
    "                # optimizerを初期化\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # 順伝搬（forward）計算\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # 順伝搬（forward）計算\n",
    "                    outputs = net(images)\n",
    "                    #print(outputs[0].type())\n",
    "                    # 損失の計算\n",
    "                    loss_l, loss_c = criterion(outputs, targets)\n",
    "                    loss = loss_l + loss_c\n",
    "\n",
    "                    # 訓練時はバックプロパゲーション\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()  # 勾配の計算\n",
    "\n",
    "                        # 勾配が大きくなりすぎると計算が不安定になるので、clipで最大でも勾配2.0に留める\n",
    "                        nn.utils.clip_grad_value_(\n",
    "                            net.parameters(), clip_value=2.0)\n",
    "\n",
    "                        optimizer.step()  # パラメータ更新\n",
    "\n",
    "                        if (iteration % 10 == 0):  # 10iterに1度、lossを表示\n",
    "                            t_iter_finish = time.time()\n",
    "                            duration = t_iter_finish - t_iter_start\n",
    "                            print('Iter {} || Loss: {:.4f} || 10iter: {:.4f} sec.'.format(\n",
    "                                iteration, loss.item(), duration))\n",
    "                            t_iter_start = time.time()\n",
    "                        # filter inf..\n",
    "                        if not loss.item() == float(\"inf\"):\n",
    "                            epoch_train_loss += loss.item()\n",
    "                        iteration += 1\n",
    "\n",
    "                    # 検証時\n",
    "                    else:\n",
    "                        if not loss.item() == float(\"inf\"):\n",
    "                            epoch_val_loss += loss.item()\n",
    "\n",
    "        # epochのphaseごとのlossと正解率\n",
    "        t_epoch_finish = time.time()\n",
    "        print('-------------')\n",
    "        print('epoch {} || Epoch_TRAIN_Loss:{:.4f} ||Epoch_VAL_Loss:{:.4f}'.format(\n",
    "            epoch+1, epoch_train_loss, epoch_val_loss))\n",
    "        print('timer:  {:.4f} sec.'.format(t_epoch_finish - t_epoch_start))\n",
    "        t_epoch_start = time.time()\n",
    "\n",
    "        # ログを保存\n",
    "        log_epoch = {'epoch': epoch+1,\n",
    "                     'train_loss': epoch_train_loss, 'val_loss': epoch_val_loss}\n",
    "        logs.append(log_epoch)\n",
    "        df = pd.DataFrame(logs)\n",
    "        df.to_csv(\"log_output.csv\")\n",
    "\n",
    "        epoch_train_loss = 0.0  # epochの損失和\n",
    "        epoch_val_loss = 0.0  # epochの損失和\n",
    "\n",
    "        # ネットワークを保存する\n",
    "        if ((epoch+1) % 5 == 0):\n",
    "            if useBiFPN:\n",
    "                word=\"BiFPN\"\n",
    "            else:\n",
    "                word=\"FPN\"\n",
    "            torch.save(net.state_dict(), 'weights/'+DATASET+\"_\"+backbone+\"_\" + str(300*scale) + \"_\" + word + \"_\" + \n",
    "                       str(epoch+1) + '.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used device: cuda:0\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 1/200\n",
      "-------------\n",
      "(train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/functional.py:2457: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/functional.py:2539: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 10 || Loss: 15.7518 || 10iter: 4.7307 sec.\n",
      "Iter 20 || Loss: 14.4122 || 10iter: 1.8921 sec.\n",
      "Iter 30 || Loss: 13.8655 || 10iter: 1.9784 sec.\n",
      "Iter 40 || Loss: 12.7425 || 10iter: 1.9309 sec.\n",
      "Iter 50 || Loss: 11.7954 || 10iter: 1.9372 sec.\n",
      "Iter 60 || Loss: 11.3030 || 10iter: 1.8859 sec.\n",
      "Iter 70 || Loss: 10.5033 || 10iter: 1.8864 sec.\n",
      "Iter 80 || Loss: 10.3696 || 10iter: 1.9323 sec.\n",
      "Iter 90 || Loss: 9.3117 || 10iter: 1.9685 sec.\n",
      "Iter 100 || Loss: 8.8864 || 10iter: 1.9152 sec.\n",
      "Iter 110 || Loss: 9.4674 || 10iter: 1.9103 sec.\n",
      "Iter 120 || Loss: 9.3020 || 10iter: 1.8883 sec.\n",
      "Iter 130 || Loss: 8.4071 || 10iter: 1.9002 sec.\n",
      "Iter 140 || Loss: 8.6193 || 10iter: 1.9468 sec.\n",
      "Iter 150 || Loss: 8.3351 || 10iter: 1.9050 sec.\n",
      "Iter 160 || Loss: 8.5078 || 10iter: 1.9140 sec.\n",
      "Iter 170 || Loss: 7.6491 || 10iter: 1.8982 sec.\n",
      "Iter 180 || Loss: 8.7354 || 10iter: 1.9168 sec.\n",
      "Iter 190 || Loss: 8.0486 || 10iter: 1.8867 sec.\n",
      "Iter 200 || Loss: 8.0660 || 10iter: 1.9110 sec.\n",
      "Iter 210 || Loss: 8.1561 || 10iter: 1.9034 sec.\n",
      "Iter 220 || Loss: 7.6240 || 10iter: 1.9517 sec.\n",
      "Iter 230 || Loss: 7.3998 || 10iter: 1.9581 sec.\n",
      "Iter 240 || Loss: 7.8412 || 10iter: 1.9302 sec.\n",
      "Iter 250 || Loss: 8.0879 || 10iter: 1.9219 sec.\n",
      "Iter 260 || Loss: 7.7194 || 10iter: 1.9054 sec.\n",
      "Iter 270 || Loss: 7.3940 || 10iter: 1.9516 sec.\n",
      "Iter 280 || Loss: 7.6676 || 10iter: 1.9248 sec.\n",
      "Iter 290 || Loss: 7.7508 || 10iter: 1.9279 sec.\n",
      "Iter 300 || Loss: 7.1962 || 10iter: 1.9403 sec.\n",
      "Iter 310 || Loss: 7.8204 || 10iter: 1.9398 sec.\n",
      "Iter 320 || Loss: 8.5559 || 10iter: 1.8789 sec.\n",
      "Iter 330 || Loss: 8.2730 || 10iter: 1.9394 sec.\n",
      "Iter 340 || Loss: 7.8743 || 10iter: 1.9160 sec.\n",
      "Iter 350 || Loss: 7.0550 || 10iter: 1.9448 sec.\n",
      "Iter 360 || Loss: 7.6786 || 10iter: 1.9152 sec.\n",
      "Iter 370 || Loss: 7.1771 || 10iter: 1.9409 sec.\n",
      "Iter 380 || Loss: 7.4616 || 10iter: 1.9308 sec.\n",
      "Iter 390 || Loss: 7.8728 || 10iter: 1.8980 sec.\n",
      "Iter 400 || Loss: 7.2624 || 10iter: 1.9601 sec.\n",
      "Iter 410 || Loss: 7.2867 || 10iter: 1.9470 sec.\n",
      "Iter 420 || Loss: 8.0774 || 10iter: 1.9203 sec.\n",
      "Iter 430 || Loss: 8.2089 || 10iter: 1.9373 sec.\n",
      "Iter 440 || Loss: 7.6132 || 10iter: 1.9623 sec.\n",
      "-------------\n",
      "epoch 1 || Epoch_TRAIN_Loss:8116.4423 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  205.5965 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 2/200\n",
      "-------------\n",
      "(train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/functional.py:2457: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/functional.py:2539: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1040 || Loss: 7.3981 || 10iter: 2.7324 sec.\n",
      "Iter 1050 || Loss: 7.1401 || 10iter: 2.0177 sec.\n",
      "Iter 1060 || Loss: 6.3296 || 10iter: 1.9268 sec.\n",
      "Iter 1070 || Loss: 6.9658 || 10iter: 1.9337 sec.\n",
      "Iter 1080 || Loss: 6.6879 || 10iter: 1.9037 sec.\n",
      "Iter 1090 || Loss: 6.2444 || 10iter: 1.9686 sec.\n",
      "Iter 1100 || Loss: 7.0513 || 10iter: 1.9276 sec.\n",
      "Iter 1110 || Loss: 6.4635 || 10iter: 1.9627 sec.\n",
      "Iter 1120 || Loss: 7.0456 || 10iter: 1.9197 sec.\n",
      "Iter 1130 || Loss: 7.0069 || 10iter: 1.8963 sec.\n",
      "Iter 1140 || Loss: 7.0612 || 10iter: 1.9401 sec.\n",
      "Iter 1150 || Loss: 5.8751 || 10iter: 1.9078 sec.\n",
      "Iter 1160 || Loss: 6.3783 || 10iter: 1.9610 sec.\n",
      "Iter 1170 || Loss: 7.1168 || 10iter: 1.8801 sec.\n",
      "Iter 1180 || Loss: 6.9482 || 10iter: 1.8925 sec.\n",
      "Iter 1190 || Loss: 6.2115 || 10iter: 1.8916 sec.\n",
      "Iter 1200 || Loss: 6.2355 || 10iter: 1.9432 sec.\n",
      "Iter 1210 || Loss: 7.4168 || 10iter: 1.9235 sec.\n",
      "Iter 1220 || Loss: 5.5916 || 10iter: 1.9344 sec.\n",
      "Iter 1230 || Loss: 6.1026 || 10iter: 1.8963 sec.\n",
      "Iter 1240 || Loss: 6.3649 || 10iter: 1.9062 sec.\n",
      "Iter 1250 || Loss: 7.0068 || 10iter: 1.9557 sec.\n",
      "Iter 1260 || Loss: 6.5162 || 10iter: 1.9458 sec.\n",
      "Iter 1270 || Loss: 6.8819 || 10iter: 1.9029 sec.\n",
      "Iter 1280 || Loss: 5.9832 || 10iter: 1.9077 sec.\n",
      "Iter 1290 || Loss: 6.6554 || 10iter: 1.9100 sec.\n",
      "Iter 1300 || Loss: 5.8981 || 10iter: 1.9431 sec.\n",
      "Iter 1310 || Loss: 6.2569 || 10iter: 1.8985 sec.\n",
      "Iter 1320 || Loss: 6.0941 || 10iter: 1.9482 sec.\n",
      "Iter 1330 || Loss: 5.7917 || 10iter: 1.9389 sec.\n",
      "Iter 1340 || Loss: 5.8668 || 10iter: 1.9393 sec.\n",
      "Iter 1350 || Loss: 6.1126 || 10iter: 1.9015 sec.\n",
      "Iter 1360 || Loss: 6.6019 || 10iter: 1.9271 sec.\n",
      "Iter 1370 || Loss: 5.9754 || 10iter: 1.9343 sec.\n",
      "Iter 1380 || Loss: 6.9415 || 10iter: 1.9116 sec.\n",
      "Iter 1390 || Loss: 6.4699 || 10iter: 1.9182 sec.\n",
      "Iter 1400 || Loss: 6.1201 || 10iter: 1.8840 sec.\n",
      "Iter 1410 || Loss: 6.9341 || 10iter: 1.9241 sec.\n",
      "Iter 1420 || Loss: 5.9466 || 10iter: 1.9144 sec.\n",
      "Iter 1430 || Loss: 6.8495 || 10iter: 1.9034 sec.\n",
      "Iter 1440 || Loss: 6.7957 || 10iter: 1.9057 sec.\n",
      "Iter 1450 || Loss: 6.3031 || 10iter: 1.9131 sec.\n",
      "Iter 1460 || Loss: 6.5252 || 10iter: 1.9576 sec.\n",
      "Iter 1470 || Loss: 6.5337 || 10iter: 1.9047 sec.\n",
      "Iter 1480 || Loss: 6.1693 || 10iter: 1.9563 sec.\n",
      "Iter 1490 || Loss: 6.4933 || 10iter: 1.9209 sec.\n",
      "Iter 1500 || Loss: 6.4583 || 10iter: 1.9039 sec.\n",
      "Iter 1510 || Loss: 5.2166 || 10iter: 1.9425 sec.\n",
      "Iter 1520 || Loss: 7.0859 || 10iter: 1.9332 sec.\n",
      "Iter 1530 || Loss: 6.1202 || 10iter: 1.8999 sec.\n",
      "Iter 1540 || Loss: 6.5705 || 10iter: 1.9209 sec.\n",
      "Iter 1550 || Loss: 6.5723 || 10iter: 1.9181 sec.\n",
      "Iter 1560 || Loss: 6.0770 || 10iter: 1.9187 sec.\n",
      "Iter 1570 || Loss: 6.5784 || 10iter: 1.9138 sec.\n",
      "Iter 1580 || Loss: 5.7582 || 10iter: 1.9248 sec.\n",
      "Iter 1590 || Loss: 6.6615 || 10iter: 1.9167 sec.\n",
      "Iter 1600 || Loss: 6.0470 || 10iter: 1.9091 sec.\n",
      "Iter 1610 || Loss: 6.4892 || 10iter: 1.9129 sec.\n",
      "Iter 1620 || Loss: 6.2785 || 10iter: 1.9371 sec.\n",
      "Iter 1630 || Loss: 6.5960 || 10iter: 1.9564 sec.\n",
      "Iter 1640 || Loss: 6.9436 || 10iter: 1.9231 sec.\n",
      "Iter 1650 || Loss: 6.5662 || 10iter: 1.9311 sec.\n",
      "Iter 1660 || Loss: 6.9712 || 10iter: 1.8700 sec.\n",
      "Iter 1670 || Loss: 6.3291 || 10iter: 1.8912 sec.\n",
      "Iter 1680 || Loss: 6.6033 || 10iter: 1.9763 sec.\n",
      "Iter 1690 || Loss: 6.3726 || 10iter: 1.9093 sec.\n",
      "Iter 1700 || Loss: 6.7758 || 10iter: 1.9224 sec.\n",
      "Iter 1710 || Loss: 6.8447 || 10iter: 1.9486 sec.\n",
      "Iter 1720 || Loss: 6.1328 || 10iter: 1.9175 sec.\n",
      "Iter 1730 || Loss: 5.9720 || 10iter: 1.9292 sec.\n",
      "Iter 1740 || Loss: 5.7320 || 10iter: 1.9601 sec.\n",
      "Iter 1750 || Loss: 6.0891 || 10iter: 1.9216 sec.\n",
      "Iter 1760 || Loss: 5.8682 || 10iter: 1.8739 sec.\n",
      "Iter 1770 || Loss: 6.8553 || 10iter: 1.9453 sec.\n",
      "Iter 1780 || Loss: 6.2981 || 10iter: 1.9153 sec.\n",
      "Iter 1790 || Loss: 6.3228 || 10iter: 1.9104 sec.\n",
      "Iter 1800 || Loss: 5.8865 || 10iter: 1.9279 sec.\n",
      "Iter 1810 || Loss: 6.5598 || 10iter: 1.9481 sec.\n",
      "Iter 1820 || Loss: 6.2339 || 10iter: 1.9025 sec.\n",
      "Iter 1830 || Loss: 6.7696 || 10iter: 1.9371 sec.\n",
      "Iter 1840 || Loss: 6.5442 || 10iter: 1.9026 sec.\n",
      "Iter 1850 || Loss: 6.1437 || 10iter: 1.9116 sec.\n",
      "Iter 1860 || Loss: 5.9688 || 10iter: 1.9156 sec.\n",
      "Iter 1870 || Loss: 6.7230 || 10iter: 1.9290 sec.\n",
      "Iter 1880 || Loss: 6.8780 || 10iter: 1.9226 sec.\n",
      "Iter 1890 || Loss: 6.5111 || 10iter: 1.9059 sec.\n",
      "Iter 1900 || Loss: 5.7888 || 10iter: 1.9135 sec.\n",
      "Iter 1910 || Loss: 6.1375 || 10iter: 1.9197 sec.\n",
      "Iter 1920 || Loss: 6.4233 || 10iter: 1.8984 sec.\n",
      "Iter 1930 || Loss: 5.9493 || 10iter: 1.9152 sec.\n",
      "Iter 1940 || Loss: 5.9321 || 10iter: 1.8934 sec.\n",
      "Iter 1950 || Loss: 5.9742 || 10iter: 1.8896 sec.\n",
      "Iter 1960 || Loss: 6.2150 || 10iter: 1.9080 sec.\n",
      "Iter 1970 || Loss: 6.3895 || 10iter: 1.9304 sec.\n",
      "Iter 1980 || Loss: 6.6977 || 10iter: 1.8916 sec.\n",
      "Iter 1990 || Loss: 6.7708 || 10iter: 1.9209 sec.\n",
      "Iter 2000 || Loss: 6.7084 || 10iter: 1.9303 sec.\n",
      "Iter 2010 || Loss: 5.2741 || 10iter: 1.9240 sec.\n",
      "Iter 2020 || Loss: 6.3070 || 10iter: 1.9108 sec.\n",
      "Iter 2030 || Loss: 6.0307 || 10iter: 1.9247 sec.\n",
      "Iter 2040 || Loss: 6.0952 || 10iter: 1.9266 sec.\n",
      "Iter 2050 || Loss: 5.7931 || 10iter: 1.8955 sec.\n",
      "Iter 2060 || Loss: 5.5522 || 10iter: 1.8569 sec.\n",
      "Iter 2070 || Loss: 6.8603 || 10iter: 1.7381 sec.\n",
      "-------------\n",
      "epoch 2 || Epoch_TRAIN_Loss:6614.8536 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  203.4751 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 3/200\n",
      "-------------\n",
      "(train)\n",
      "Iter 2080 || Loss: 6.2009 || 10iter: 3.6872 sec.\n",
      "Iter 2090 || Loss: 6.2204 || 10iter: 1.8831 sec.\n",
      "Iter 2100 || Loss: 5.9224 || 10iter: 1.9122 sec.\n",
      "Iter 2110 || Loss: 6.6301 || 10iter: 1.9271 sec.\n",
      "Iter 2120 || Loss: 5.7328 || 10iter: 1.8922 sec.\n",
      "Iter 2130 || Loss: 6.5620 || 10iter: 1.9473 sec.\n",
      "Iter 2140 || Loss: 5.9839 || 10iter: 1.9144 sec.\n",
      "Iter 2150 || Loss: 5.7984 || 10iter: 1.9296 sec.\n",
      "Iter 2160 || Loss: 6.9789 || 10iter: 1.8936 sec.\n",
      "Iter 2170 || Loss: 4.9826 || 10iter: 1.9362 sec.\n",
      "Iter 2180 || Loss: 6.0308 || 10iter: 1.9365 sec.\n",
      "Iter 2190 || Loss: 6.1750 || 10iter: 1.9060 sec.\n",
      "Iter 2200 || Loss: 6.1274 || 10iter: 1.8972 sec.\n",
      "Iter 2210 || Loss: 6.7538 || 10iter: 1.8774 sec.\n",
      "Iter 2220 || Loss: 6.1418 || 10iter: 1.9135 sec.\n",
      "Iter 2230 || Loss: 5.3181 || 10iter: 1.9372 sec.\n",
      "Iter 2240 || Loss: 6.2427 || 10iter: 1.9094 sec.\n",
      "Iter 2250 || Loss: 5.8552 || 10iter: 1.8982 sec.\n",
      "Iter 2260 || Loss: 5.5481 || 10iter: 1.9138 sec.\n",
      "Iter 2270 || Loss: 6.5120 || 10iter: 1.9137 sec.\n",
      "Iter 2280 || Loss: 6.2690 || 10iter: 1.9085 sec.\n",
      "Iter 2290 || Loss: 5.8348 || 10iter: 1.9713 sec.\n",
      "Iter 2300 || Loss: 5.4405 || 10iter: 1.9432 sec.\n",
      "Iter 2310 || Loss: 6.1716 || 10iter: 1.9180 sec.\n",
      "Iter 2320 || Loss: 5.8104 || 10iter: 1.9229 sec.\n",
      "Iter 2330 || Loss: 5.9316 || 10iter: 1.9480 sec.\n",
      "Iter 2340 || Loss: 6.2018 || 10iter: 1.9445 sec.\n",
      "Iter 2350 || Loss: 6.0464 || 10iter: 1.9033 sec.\n",
      "Iter 2360 || Loss: 5.8741 || 10iter: 1.9045 sec.\n",
      "Iter 2370 || Loss: 5.9396 || 10iter: 1.9461 sec.\n",
      "Iter 2380 || Loss: 6.6027 || 10iter: 1.9085 sec.\n",
      "Iter 2390 || Loss: 5.5763 || 10iter: 1.9209 sec.\n",
      "Iter 2400 || Loss: 5.9338 || 10iter: 1.9226 sec.\n",
      "Iter 2410 || Loss: 5.6852 || 10iter: 1.9464 sec.\n",
      "Iter 2420 || Loss: 5.7879 || 10iter: 1.9467 sec.\n",
      "Iter 2430 || Loss: 6.4833 || 10iter: 1.8930 sec.\n",
      "Iter 2440 || Loss: 6.1358 || 10iter: 1.9259 sec.\n",
      "Iter 2450 || Loss: 6.0421 || 10iter: 1.9404 sec.\n",
      "Iter 2460 || Loss: 6.2651 || 10iter: 1.8849 sec.\n",
      "Iter 2470 || Loss: 6.2316 || 10iter: 1.9379 sec.\n",
      "Iter 2480 || Loss: 6.6244 || 10iter: 1.9085 sec.\n",
      "Iter 2490 || Loss: 5.9621 || 10iter: 1.9359 sec.\n",
      "Iter 2500 || Loss: 5.7220 || 10iter: 1.9060 sec.\n",
      "Iter 2510 || Loss: 5.5667 || 10iter: 1.8986 sec.\n",
      "Iter 2520 || Loss: 5.7661 || 10iter: 1.9421 sec.\n",
      "Iter 2530 || Loss: 6.2610 || 10iter: 1.9490 sec.\n",
      "Iter 2540 || Loss: 6.1295 || 10iter: 2.0034 sec.\n",
      "Iter 2550 || Loss: 6.2075 || 10iter: 1.9340 sec.\n",
      "Iter 2560 || Loss: 5.7161 || 10iter: 1.9302 sec.\n",
      "Iter 2570 || Loss: 6.1298 || 10iter: 1.9426 sec.\n",
      "Iter 2580 || Loss: 5.1992 || 10iter: 1.9298 sec.\n",
      "Iter 2590 || Loss: 5.6399 || 10iter: 1.9380 sec.\n",
      "Iter 2600 || Loss: 6.2890 || 10iter: 1.9024 sec.\n",
      "Iter 2610 || Loss: 6.1346 || 10iter: 1.8967 sec.\n",
      "Iter 2620 || Loss: 6.6686 || 10iter: 1.9314 sec.\n",
      "Iter 2630 || Loss: 5.5704 || 10iter: 1.9538 sec.\n",
      "Iter 2640 || Loss: 6.2783 || 10iter: 1.9141 sec.\n",
      "Iter 2650 || Loss: 6.0719 || 10iter: 1.9036 sec.\n",
      "Iter 2660 || Loss: 5.9588 || 10iter: 1.8919 sec.\n",
      "Iter 2670 || Loss: 5.5576 || 10iter: 1.9184 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 2680 || Loss: 6.6553 || 10iter: 1.9460 sec.\n",
      "Iter 2690 || Loss: 6.7202 || 10iter: 1.8867 sec.\n",
      "Iter 2700 || Loss: 5.9778 || 10iter: 1.8967 sec.\n",
      "Iter 2710 || Loss: 6.0141 || 10iter: 1.9559 sec.\n",
      "Iter 2720 || Loss: 5.4180 || 10iter: 1.9364 sec.\n",
      "Iter 2730 || Loss: 6.5242 || 10iter: 1.9199 sec.\n",
      "Iter 2740 || Loss: 6.6022 || 10iter: 1.9561 sec.\n",
      "Iter 2750 || Loss: 5.9271 || 10iter: 1.9231 sec.\n",
      "Iter 2760 || Loss: 6.3372 || 10iter: 1.9598 sec.\n",
      "Iter 2770 || Loss: 5.7391 || 10iter: 1.9252 sec.\n",
      "Iter 2780 || Loss: 6.3043 || 10iter: 1.8984 sec.\n",
      "Iter 2790 || Loss: 7.3497 || 10iter: 1.9225 sec.\n",
      "Iter 2800 || Loss: 5.5991 || 10iter: 1.9239 sec.\n",
      "Iter 2810 || Loss: 5.7196 || 10iter: 1.9760 sec.\n",
      "Iter 2820 || Loss: 6.8417 || 10iter: 1.9412 sec.\n",
      "Iter 2830 || Loss: 6.6544 || 10iter: 1.9212 sec.\n",
      "Iter 2840 || Loss: 5.4433 || 10iter: 1.9410 sec.\n",
      "Iter 2850 || Loss: 5.5030 || 10iter: 1.8911 sec.\n",
      "Iter 2860 || Loss: 5.8773 || 10iter: 1.9169 sec.\n",
      "Iter 2870 || Loss: 6.0134 || 10iter: 1.9294 sec.\n",
      "Iter 2880 || Loss: 5.5524 || 10iter: 1.9469 sec.\n",
      "Iter 2890 || Loss: 5.7362 || 10iter: 1.8994 sec.\n",
      "Iter 2900 || Loss: 5.5710 || 10iter: 1.8991 sec.\n",
      "Iter 2910 || Loss: 5.6592 || 10iter: 1.9354 sec.\n",
      "Iter 2920 || Loss: 5.2004 || 10iter: 1.9027 sec.\n",
      "Iter 2930 || Loss: 6.5301 || 10iter: 1.9168 sec.\n",
      "Iter 2940 || Loss: 6.3754 || 10iter: 1.9007 sec.\n",
      "Iter 2950 || Loss: 5.2248 || 10iter: 1.8894 sec.\n",
      "Iter 2960 || Loss: 5.4470 || 10iter: 1.9255 sec.\n",
      "Iter 2970 || Loss: 6.0676 || 10iter: 1.9486 sec.\n",
      "Iter 2980 || Loss: 6.2856 || 10iter: 1.9066 sec.\n",
      "Iter 2990 || Loss: 5.8748 || 10iter: 1.9250 sec.\n",
      "Iter 3000 || Loss: 5.8902 || 10iter: 1.9392 sec.\n",
      "Iter 3010 || Loss: 5.7938 || 10iter: 1.9204 sec.\n",
      "Iter 3020 || Loss: 6.1163 || 10iter: 1.8698 sec.\n",
      "Iter 3030 || Loss: 6.0617 || 10iter: 1.8985 sec.\n",
      "Iter 3040 || Loss: 6.3836 || 10iter: 1.8962 sec.\n",
      "Iter 3050 || Loss: 6.4083 || 10iter: 1.9479 sec.\n",
      "Iter 3060 || Loss: 6.1602 || 10iter: 1.9320 sec.\n",
      "Iter 3070 || Loss: 7.1081 || 10iter: 1.8975 sec.\n",
      "Iter 3080 || Loss: 5.8629 || 10iter: 1.9246 sec.\n",
      "Iter 3090 || Loss: 6.1259 || 10iter: 1.9222 sec.\n",
      "Iter 3100 || Loss: 5.5650 || 10iter: 1.8193 sec.\n",
      "-------------\n",
      "epoch 3 || Epoch_TRAIN_Loss:6204.8702 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  203.5415 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 4/200\n",
      "-------------\n",
      "(train)\n",
      "Iter 3110 || Loss: 5.8793 || 10iter: 2.6717 sec.\n",
      "Iter 3120 || Loss: 6.2001 || 10iter: 1.9694 sec.\n",
      "Iter 3130 || Loss: 6.0468 || 10iter: 1.9720 sec.\n",
      "Iter 3140 || Loss: 6.4812 || 10iter: 1.9011 sec.\n",
      "Iter 3150 || Loss: 5.3816 || 10iter: 1.9075 sec.\n",
      "Iter 3160 || Loss: 5.7660 || 10iter: 1.9035 sec.\n",
      "Iter 3170 || Loss: 5.7194 || 10iter: 1.9134 sec.\n",
      "Iter 3180 || Loss: 5.7383 || 10iter: 1.9536 sec.\n",
      "Iter 3190 || Loss: 6.3789 || 10iter: 1.9037 sec.\n",
      "Iter 3200 || Loss: 5.7865 || 10iter: 1.9110 sec.\n",
      "Iter 3210 || Loss: 5.9149 || 10iter: 1.9037 sec.\n",
      "Iter 3220 || Loss: 5.3373 || 10iter: 1.9284 sec.\n",
      "Iter 3230 || Loss: 6.3403 || 10iter: 1.9133 sec.\n",
      "Iter 3240 || Loss: 6.0725 || 10iter: 1.9351 sec.\n",
      "Iter 3250 || Loss: 5.4783 || 10iter: 1.9017 sec.\n",
      "Iter 3260 || Loss: 5.8148 || 10iter: 1.9476 sec.\n",
      "Iter 3270 || Loss: 5.7243 || 10iter: 1.8892 sec.\n",
      "Iter 3280 || Loss: 5.2606 || 10iter: 1.9149 sec.\n",
      "Iter 3290 || Loss: 6.1021 || 10iter: 1.9236 sec.\n",
      "Iter 3300 || Loss: 5.0021 || 10iter: 1.8952 sec.\n",
      "Iter 3310 || Loss: 5.6538 || 10iter: 1.9349 sec.\n",
      "Iter 3320 || Loss: 5.8735 || 10iter: 1.9225 sec.\n",
      "Iter 3330 || Loss: 6.0720 || 10iter: 1.9147 sec.\n",
      "Iter 3340 || Loss: 5.7793 || 10iter: 1.9126 sec.\n",
      "Iter 3350 || Loss: 5.6325 || 10iter: 1.9483 sec.\n",
      "Iter 3360 || Loss: 5.9388 || 10iter: 1.9324 sec.\n",
      "Iter 3370 || Loss: 5.5870 || 10iter: 1.9048 sec.\n",
      "Iter 3380 || Loss: 6.0012 || 10iter: 1.9528 sec.\n",
      "Iter 3390 || Loss: 6.2153 || 10iter: 1.9066 sec.\n",
      "Iter 3400 || Loss: 5.9682 || 10iter: 1.9020 sec.\n",
      "Iter 3410 || Loss: 6.5897 || 10iter: 1.9316 sec.\n",
      "Iter 3420 || Loss: 5.0391 || 10iter: 1.9382 sec.\n",
      "Iter 3430 || Loss: 5.4903 || 10iter: 1.9828 sec.\n",
      "Iter 3440 || Loss: 5.5313 || 10iter: 1.9535 sec.\n",
      "Iter 3450 || Loss: 5.9243 || 10iter: 2.0076 sec.\n",
      "Iter 3460 || Loss: 5.4482 || 10iter: 1.9587 sec.\n",
      "Iter 3470 || Loss: 5.3784 || 10iter: 1.9217 sec.\n",
      "Iter 3480 || Loss: 5.3976 || 10iter: 1.9065 sec.\n",
      "Iter 3490 || Loss: 5.4845 || 10iter: 1.9064 sec.\n",
      "Iter 3500 || Loss: 5.3729 || 10iter: 1.9155 sec.\n",
      "Iter 3510 || Loss: 6.0532 || 10iter: 1.9102 sec.\n",
      "Iter 3520 || Loss: 6.5765 || 10iter: 1.9432 sec.\n",
      "Iter 3530 || Loss: 5.6584 || 10iter: 1.9218 sec.\n",
      "Iter 3540 || Loss: 5.0806 || 10iter: 1.9423 sec.\n",
      "Iter 3550 || Loss: 5.7217 || 10iter: 1.9580 sec.\n",
      "Iter 3560 || Loss: 4.9283 || 10iter: 1.9375 sec.\n",
      "Iter 3570 || Loss: 6.1907 || 10iter: 1.9083 sec.\n",
      "Iter 3580 || Loss: 5.3850 || 10iter: 1.8684 sec.\n",
      "Iter 3590 || Loss: 4.9205 || 10iter: 1.9359 sec.\n",
      "Iter 3600 || Loss: 6.2988 || 10iter: 1.8874 sec.\n",
      "Iter 3610 || Loss: 5.9377 || 10iter: 1.9525 sec.\n",
      "Iter 3620 || Loss: 5.9317 || 10iter: 1.9065 sec.\n",
      "Iter 3630 || Loss: 5.4728 || 10iter: 1.9354 sec.\n",
      "Iter 3640 || Loss: 6.1968 || 10iter: 1.9133 sec.\n",
      "Iter 3650 || Loss: 5.3487 || 10iter: 1.8898 sec.\n",
      "Iter 3660 || Loss: 5.8080 || 10iter: 1.9198 sec.\n",
      "Iter 3670 || Loss: 6.0591 || 10iter: 1.9011 sec.\n",
      "Iter 3680 || Loss: 5.8152 || 10iter: 1.9185 sec.\n",
      "Iter 3690 || Loss: 5.7057 || 10iter: 1.8776 sec.\n",
      "Iter 3700 || Loss: 6.3647 || 10iter: 1.8982 sec.\n",
      "Iter 3710 || Loss: 5.8901 || 10iter: 1.9278 sec.\n",
      "Iter 3720 || Loss: 5.5554 || 10iter: 1.9389 sec.\n",
      "Iter 3730 || Loss: 5.1569 || 10iter: 1.9105 sec.\n",
      "Iter 3740 || Loss: 6.7121 || 10iter: 1.9014 sec.\n",
      "Iter 3750 || Loss: 5.8162 || 10iter: 1.9498 sec.\n",
      "Iter 3760 || Loss: 5.6945 || 10iter: 1.9121 sec.\n",
      "Iter 3770 || Loss: 6.0207 || 10iter: 1.9130 sec.\n",
      "Iter 3780 || Loss: 6.3730 || 10iter: 1.9626 sec.\n",
      "Iter 3790 || Loss: 6.0466 || 10iter: 1.9721 sec.\n",
      "Iter 3800 || Loss: 5.9875 || 10iter: 1.9130 sec.\n",
      "Iter 3810 || Loss: 5.1606 || 10iter: 1.9301 sec.\n",
      "Iter 3820 || Loss: 5.8891 || 10iter: 1.8851 sec.\n",
      "Iter 3830 || Loss: 6.2091 || 10iter: 1.9202 sec.\n",
      "Iter 3840 || Loss: 5.5418 || 10iter: 1.9226 sec.\n",
      "Iter 3850 || Loss: 5.3073 || 10iter: 1.9183 sec.\n",
      "Iter 3860 || Loss: 5.8605 || 10iter: 1.9406 sec.\n",
      "Iter 3870 || Loss: 5.7970 || 10iter: 1.8944 sec.\n",
      "Iter 3880 || Loss: 6.1666 || 10iter: 1.9309 sec.\n",
      "Iter 3890 || Loss: 5.5670 || 10iter: 1.9488 sec.\n",
      "Iter 3900 || Loss: 5.3022 || 10iter: 1.8933 sec.\n",
      "Iter 3910 || Loss: 6.0552 || 10iter: 1.9317 sec.\n",
      "Iter 3920 || Loss: 6.4889 || 10iter: 1.9128 sec.\n",
      "Iter 3930 || Loss: 5.7632 || 10iter: 1.8857 sec.\n",
      "Iter 3940 || Loss: 5.3335 || 10iter: 1.8960 sec.\n",
      "Iter 3950 || Loss: 5.7937 || 10iter: 1.9391 sec.\n",
      "Iter 3960 || Loss: 5.7296 || 10iter: 1.9269 sec.\n",
      "Iter 3970 || Loss: 5.5872 || 10iter: 1.9561 sec.\n",
      "Iter 3980 || Loss: 5.4882 || 10iter: 1.9564 sec.\n",
      "Iter 3990 || Loss: 5.9010 || 10iter: 1.9668 sec.\n",
      "Iter 4000 || Loss: 6.2337 || 10iter: 1.9490 sec.\n",
      "Iter 4010 || Loss: 5.4732 || 10iter: 1.9008 sec.\n",
      "Iter 4020 || Loss: 5.3963 || 10iter: 1.9346 sec.\n",
      "Iter 4030 || Loss: 5.3623 || 10iter: 1.8886 sec.\n",
      "Iter 4040 || Loss: 5.6274 || 10iter: 1.8993 sec.\n",
      "Iter 4050 || Loss: 4.8907 || 10iter: 1.9147 sec.\n",
      "Iter 4060 || Loss: 5.8309 || 10iter: 1.9083 sec.\n",
      "Iter 4070 || Loss: 6.3519 || 10iter: 1.9104 sec.\n",
      "Iter 4080 || Loss: 6.0935 || 10iter: 1.9479 sec.\n",
      "Iter 4090 || Loss: 5.6040 || 10iter: 1.9521 sec.\n",
      "Iter 4100 || Loss: 6.6016 || 10iter: 1.9053 sec.\n",
      "Iter 4110 || Loss: 5.8515 || 10iter: 1.9514 sec.\n",
      "Iter 4120 || Loss: 5.4065 || 10iter: 1.8861 sec.\n",
      "Iter 4130 || Loss: 5.9942 || 10iter: 1.8570 sec.\n",
      "Iter 4140 || Loss: 4.9276 || 10iter: 1.7437 sec.\n",
      "-------------\n",
      "epoch 4 || Epoch_TRAIN_Loss:5950.0694 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  203.6692 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 5/200\n",
      "-------------\n",
      "(train)\n",
      "Iter 4150 || Loss: 5.7835 || 10iter: 3.7371 sec.\n",
      "Iter 4160 || Loss: 5.4967 || 10iter: 1.8938 sec.\n",
      "Iter 4170 || Loss: 5.3168 || 10iter: 1.9162 sec.\n",
      "Iter 4180 || Loss: 6.4354 || 10iter: 1.8962 sec.\n",
      "Iter 4190 || Loss: 5.3139 || 10iter: 1.8956 sec.\n",
      "Iter 4200 || Loss: 4.8802 || 10iter: 1.9128 sec.\n",
      "Iter 4210 || Loss: 5.8364 || 10iter: 1.9055 sec.\n",
      "Iter 4220 || Loss: 5.2796 || 10iter: 1.9006 sec.\n",
      "Iter 4230 || Loss: 5.7652 || 10iter: 1.9082 sec.\n",
      "Iter 4240 || Loss: 5.2618 || 10iter: 1.9158 sec.\n",
      "Iter 4250 || Loss: 6.0142 || 10iter: 1.8908 sec.\n",
      "Iter 4260 || Loss: 5.1867 || 10iter: 1.8945 sec.\n",
      "Iter 4270 || Loss: 5.3560 || 10iter: 1.8920 sec.\n",
      "Iter 4280 || Loss: 5.4599 || 10iter: 1.9221 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 4290 || Loss: 5.1026 || 10iter: 1.9341 sec.\n",
      "Iter 4300 || Loss: 5.7481 || 10iter: 1.9045 sec.\n",
      "Iter 4310 || Loss: 5.5600 || 10iter: 1.9480 sec.\n",
      "Iter 4320 || Loss: 5.7486 || 10iter: 1.9326 sec.\n",
      "Iter 4330 || Loss: 6.1395 || 10iter: 1.9198 sec.\n",
      "Iter 4340 || Loss: 5.8731 || 10iter: 1.9244 sec.\n",
      "Iter 4350 || Loss: 5.4092 || 10iter: 2.0157 sec.\n",
      "Iter 4360 || Loss: 5.3820 || 10iter: 2.0196 sec.\n",
      "Iter 4370 || Loss: 5.5349 || 10iter: 2.0085 sec.\n",
      "Iter 4380 || Loss: 6.0740 || 10iter: 1.9467 sec.\n",
      "Iter 4390 || Loss: 5.1966 || 10iter: 1.9536 sec.\n",
      "Iter 4400 || Loss: 5.6187 || 10iter: 1.9000 sec.\n",
      "Iter 4410 || Loss: 5.6328 || 10iter: 1.9419 sec.\n",
      "Iter 4420 || Loss: 5.4136 || 10iter: 1.9053 sec.\n",
      "Iter 4430 || Loss: 5.2870 || 10iter: 1.9082 sec.\n",
      "Iter 4440 || Loss: 5.1681 || 10iter: 1.8873 sec.\n",
      "Iter 4450 || Loss: 5.4315 || 10iter: 1.9121 sec.\n",
      "Iter 4460 || Loss: 5.6397 || 10iter: 1.9025 sec.\n",
      "Iter 4470 || Loss: 5.5429 || 10iter: 1.9126 sec.\n",
      "Iter 4480 || Loss: 5.7347 || 10iter: 1.9035 sec.\n",
      "Iter 4490 || Loss: 4.9496 || 10iter: 1.9348 sec.\n",
      "Iter 4500 || Loss: 5.8241 || 10iter: 1.9303 sec.\n",
      "Iter 4510 || Loss: 5.3387 || 10iter: 1.9374 sec.\n",
      "Iter 4520 || Loss: 6.2921 || 10iter: 1.9568 sec.\n",
      "Iter 4530 || Loss: 5.2400 || 10iter: 1.9184 sec.\n",
      "Iter 4540 || Loss: 5.4240 || 10iter: 1.8974 sec.\n",
      "Iter 4550 || Loss: 5.2452 || 10iter: 1.8924 sec.\n",
      "Iter 4560 || Loss: 5.3237 || 10iter: 1.9101 sec.\n",
      "Iter 4570 || Loss: 5.0983 || 10iter: 1.9800 sec.\n",
      "Iter 4580 || Loss: 5.6004 || 10iter: 1.9079 sec.\n",
      "Iter 4590 || Loss: 6.0337 || 10iter: 1.9064 sec.\n",
      "Iter 4600 || Loss: 5.4315 || 10iter: 1.9183 sec.\n",
      "Iter 4610 || Loss: 6.3973 || 10iter: 1.9171 sec.\n",
      "Iter 4620 || Loss: 5.5012 || 10iter: 1.9081 sec.\n",
      "Iter 4630 || Loss: 5.6937 || 10iter: 1.9125 sec.\n",
      "Iter 4640 || Loss: 5.9823 || 10iter: 1.9130 sec.\n",
      "Iter 4650 || Loss: 5.7043 || 10iter: 1.9059 sec.\n",
      "Iter 4660 || Loss: 5.5554 || 10iter: 1.9564 sec.\n",
      "Iter 4670 || Loss: 6.3228 || 10iter: 1.8811 sec.\n",
      "Iter 4680 || Loss: 5.2833 || 10iter: 1.9029 sec.\n",
      "Iter 4690 || Loss: 5.4958 || 10iter: 1.9139 sec.\n",
      "Iter 4700 || Loss: 5.6965 || 10iter: 1.9336 sec.\n",
      "Iter 4710 || Loss: 5.7161 || 10iter: 1.9094 sec.\n",
      "Iter 4720 || Loss: 5.4580 || 10iter: 1.9110 sec.\n",
      "Iter 4730 || Loss: 5.3730 || 10iter: 1.9265 sec.\n",
      "Iter 4740 || Loss: 5.2783 || 10iter: 1.9118 sec.\n",
      "Iter 4750 || Loss: 5.8961 || 10iter: 1.9264 sec.\n",
      "Iter 4760 || Loss: 5.4321 || 10iter: 1.9768 sec.\n",
      "Iter 4770 || Loss: 5.4692 || 10iter: 1.8904 sec.\n",
      "Iter 4780 || Loss: 5.1850 || 10iter: 1.9236 sec.\n",
      "Iter 4790 || Loss: 5.8611 || 10iter: 1.9340 sec.\n",
      "Iter 4800 || Loss: 5.7889 || 10iter: 2.0052 sec.\n",
      "Iter 4810 || Loss: 5.7990 || 10iter: 1.9305 sec.\n",
      "Iter 4820 || Loss: 5.3129 || 10iter: 1.9487 sec.\n",
      "Iter 4830 || Loss: 5.3679 || 10iter: 1.9505 sec.\n",
      "Iter 4840 || Loss: 5.5169 || 10iter: 1.9072 sec.\n",
      "Iter 4850 || Loss: 6.1362 || 10iter: 1.9469 sec.\n",
      "Iter 4860 || Loss: 5.4825 || 10iter: 1.9287 sec.\n",
      "Iter 4870 || Loss: 5.4229 || 10iter: 1.9047 sec.\n",
      "Iter 4880 || Loss: 5.0863 || 10iter: 1.9454 sec.\n",
      "Iter 4890 || Loss: 5.5825 || 10iter: 1.9071 sec.\n",
      "Iter 4900 || Loss: 5.7294 || 10iter: 1.9023 sec.\n",
      "Iter 4910 || Loss: 5.8915 || 10iter: 1.9258 sec.\n",
      "Iter 4920 || Loss: 4.8500 || 10iter: 1.9195 sec.\n",
      "Iter 4930 || Loss: 5.5978 || 10iter: 1.8760 sec.\n",
      "Iter 4940 || Loss: 4.6089 || 10iter: 1.9476 sec.\n",
      "Iter 4950 || Loss: 5.9339 || 10iter: 1.9312 sec.\n",
      "Iter 4960 || Loss: 5.0940 || 10iter: 1.9334 sec.\n",
      "Iter 4970 || Loss: 5.6068 || 10iter: 1.9421 sec.\n",
      "Iter 4980 || Loss: 5.7186 || 10iter: 1.9092 sec.\n",
      "Iter 4990 || Loss: 5.8860 || 10iter: 1.9578 sec.\n",
      "Iter 5000 || Loss: 5.8620 || 10iter: 1.8596 sec.\n",
      "Iter 5010 || Loss: 5.1140 || 10iter: 1.9097 sec.\n",
      "Iter 5020 || Loss: 5.2869 || 10iter: 1.8951 sec.\n",
      "Iter 5030 || Loss: 5.8781 || 10iter: 1.9597 sec.\n",
      "Iter 5040 || Loss: 5.7860 || 10iter: 1.9091 sec.\n",
      "Iter 5050 || Loss: 5.1871 || 10iter: 1.9201 sec.\n",
      "Iter 5060 || Loss: 5.4153 || 10iter: 1.9224 sec.\n",
      "Iter 5070 || Loss: 5.3042 || 10iter: 1.9205 sec.\n",
      "Iter 5080 || Loss: 5.1713 || 10iter: 1.8786 sec.\n",
      "Iter 5090 || Loss: 5.9232 || 10iter: 1.9183 sec.\n",
      "Iter 5100 || Loss: 6.3301 || 10iter: 1.9304 sec.\n",
      "Iter 5110 || Loss: 5.3224 || 10iter: 1.9140 sec.\n",
      "Iter 5120 || Loss: 5.7514 || 10iter: 1.9121 sec.\n",
      "Iter 5130 || Loss: 5.0587 || 10iter: 1.9115 sec.\n",
      "Iter 5140 || Loss: 5.2688 || 10iter: 1.9375 sec.\n",
      "Iter 5150 || Loss: 5.6588 || 10iter: 1.9162 sec.\n",
      "Iter 5160 || Loss: 5.2459 || 10iter: 1.9397 sec.\n",
      "Iter 5170 || Loss: 5.4048 || 10iter: 1.8131 sec.\n",
      "-------------\n",
      "epoch 5 || Epoch_TRAIN_Loss:5790.5436 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  203.5833 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 6/200\n",
      "-------------\n",
      "(train)\n",
      "Iter 5180 || Loss: 5.5638 || 10iter: 2.7937 sec.\n",
      "Iter 5190 || Loss: 5.8528 || 10iter: 1.9518 sec.\n",
      "Iter 5200 || Loss: 4.8745 || 10iter: 1.9240 sec.\n",
      "Iter 5210 || Loss: 6.0627 || 10iter: 1.9147 sec.\n",
      "Iter 5220 || Loss: 5.6845 || 10iter: 1.9163 sec.\n",
      "Iter 5230 || Loss: 5.3720 || 10iter: 1.9192 sec.\n",
      "Iter 5240 || Loss: 6.4919 || 10iter: 1.8958 sec.\n",
      "Iter 5250 || Loss: 5.6495 || 10iter: 1.9123 sec.\n",
      "Iter 5260 || Loss: 6.2360 || 10iter: 1.9169 sec.\n",
      "Iter 5270 || Loss: 6.2473 || 10iter: 1.8988 sec.\n",
      "Iter 5280 || Loss: 5.8871 || 10iter: 1.9522 sec.\n",
      "Iter 5290 || Loss: 4.9475 || 10iter: 1.9183 sec.\n",
      "Iter 5300 || Loss: 5.1125 || 10iter: 1.9202 sec.\n",
      "Iter 5310 || Loss: 6.0746 || 10iter: 1.8638 sec.\n",
      "Iter 5320 || Loss: 5.5862 || 10iter: 1.8918 sec.\n",
      "Iter 5330 || Loss: 5.3952 || 10iter: 1.8982 sec.\n",
      "Iter 5340 || Loss: 4.7200 || 10iter: 1.9306 sec.\n",
      "Iter 5350 || Loss: 5.0349 || 10iter: 1.9348 sec.\n",
      "Iter 5360 || Loss: 6.1627 || 10iter: 1.9330 sec.\n",
      "Iter 5370 || Loss: 6.4506 || 10iter: 1.9468 sec.\n",
      "Iter 5380 || Loss: 5.2366 || 10iter: 1.9015 sec.\n",
      "Iter 5390 || Loss: 5.9505 || 10iter: 1.9157 sec.\n",
      "Iter 5400 || Loss: 5.3242 || 10iter: 1.9716 sec.\n",
      "Iter 5410 || Loss: 5.3698 || 10iter: 1.9072 sec.\n",
      "Iter 5420 || Loss: 5.4650 || 10iter: 1.8689 sec.\n",
      "Iter 5430 || Loss: 5.1035 || 10iter: 1.8852 sec.\n",
      "Iter 5440 || Loss: 5.1974 || 10iter: 1.8970 sec.\n",
      "Iter 5450 || Loss: 5.2812 || 10iter: 1.9002 sec.\n",
      "Iter 5460 || Loss: 5.8294 || 10iter: 1.9487 sec.\n",
      "Iter 5470 || Loss: 5.3021 || 10iter: 1.9577 sec.\n",
      "Iter 5480 || Loss: 5.2302 || 10iter: 1.9784 sec.\n",
      "Iter 5490 || Loss: 5.1360 || 10iter: 1.9248 sec.\n",
      "Iter 5500 || Loss: 5.3144 || 10iter: 1.9104 sec.\n",
      "Iter 5510 || Loss: 6.2622 || 10iter: 1.9346 sec.\n",
      "Iter 5520 || Loss: 5.6689 || 10iter: 1.9643 sec.\n",
      "Iter 5530 || Loss: 5.3527 || 10iter: 1.9297 sec.\n",
      "Iter 5540 || Loss: 4.9406 || 10iter: 1.8663 sec.\n",
      "Iter 5550 || Loss: 6.1685 || 10iter: 1.9012 sec.\n",
      "Iter 5560 || Loss: 5.5947 || 10iter: 1.8669 sec.\n",
      "Iter 5570 || Loss: 5.0937 || 10iter: 1.8930 sec.\n",
      "Iter 5580 || Loss: 5.1344 || 10iter: 1.9181 sec.\n",
      "Iter 5590 || Loss: 5.3126 || 10iter: 1.9355 sec.\n",
      "Iter 5600 || Loss: 5.6176 || 10iter: 1.9327 sec.\n",
      "Iter 5610 || Loss: 5.2543 || 10iter: 1.9238 sec.\n",
      "Iter 5620 || Loss: 6.3741 || 10iter: 1.9597 sec.\n",
      "Iter 5630 || Loss: 5.0453 || 10iter: 1.8962 sec.\n",
      "Iter 5640 || Loss: 5.5964 || 10iter: 1.9427 sec.\n",
      "Iter 5650 || Loss: 5.7842 || 10iter: 1.9593 sec.\n",
      "Iter 5660 || Loss: 4.8601 || 10iter: 1.9253 sec.\n",
      "Iter 5670 || Loss: 6.0115 || 10iter: 1.9005 sec.\n",
      "Iter 5680 || Loss: 5.8189 || 10iter: 1.9070 sec.\n",
      "Iter 5690 || Loss: 5.5126 || 10iter: 1.9451 sec.\n",
      "Iter 5700 || Loss: 4.3626 || 10iter: 1.9566 sec.\n",
      "Iter 5710 || Loss: 4.9804 || 10iter: 1.8964 sec.\n",
      "Iter 5720 || Loss: 4.8742 || 10iter: 1.8785 sec.\n",
      "Iter 5730 || Loss: 5.2110 || 10iter: 1.9027 sec.\n",
      "Iter 5740 || Loss: 4.7075 || 10iter: 1.9507 sec.\n",
      "Iter 5750 || Loss: 5.8188 || 10iter: 1.8942 sec.\n",
      "Iter 5760 || Loss: 5.1781 || 10iter: 1.9153 sec.\n",
      "Iter 5770 || Loss: 6.0694 || 10iter: 1.9613 sec.\n",
      "Iter 5780 || Loss: 5.1264 || 10iter: 1.9148 sec.\n",
      "Iter 5790 || Loss: 5.1921 || 10iter: 1.9229 sec.\n",
      "Iter 5800 || Loss: 5.3062 || 10iter: 1.9264 sec.\n",
      "Iter 5810 || Loss: 5.9371 || 10iter: 1.9243 sec.\n",
      "Iter 5820 || Loss: 6.5336 || 10iter: 1.9147 sec.\n",
      "Iter 5830 || Loss: 5.8122 || 10iter: 1.9053 sec.\n",
      "Iter 5840 || Loss: 5.5347 || 10iter: 1.9658 sec.\n",
      "Iter 5850 || Loss: 5.6121 || 10iter: 1.9831 sec.\n",
      "Iter 5860 || Loss: 5.2309 || 10iter: 1.9376 sec.\n",
      "Iter 5870 || Loss: 5.7739 || 10iter: 1.9340 sec.\n",
      "Iter 5880 || Loss: 4.7775 || 10iter: 1.9409 sec.\n",
      "Iter 5890 || Loss: 5.3465 || 10iter: 1.9388 sec.\n",
      "Iter 5900 || Loss: 5.0776 || 10iter: 1.9280 sec.\n",
      "Iter 5910 || Loss: 4.8831 || 10iter: 1.9094 sec.\n",
      "Iter 5920 || Loss: 5.1110 || 10iter: 1.9068 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 5930 || Loss: 5.4469 || 10iter: 1.9012 sec.\n",
      "Iter 5940 || Loss: 5.4560 || 10iter: 1.9149 sec.\n",
      "Iter 5950 || Loss: 5.1948 || 10iter: 1.9584 sec.\n",
      "Iter 5960 || Loss: 5.2100 || 10iter: 1.9072 sec.\n",
      "Iter 5970 || Loss: 5.4193 || 10iter: 1.9015 sec.\n",
      "Iter 5980 || Loss: 5.8716 || 10iter: 1.9516 sec.\n",
      "Iter 5990 || Loss: 5.2573 || 10iter: 1.9030 sec.\n",
      "Iter 6000 || Loss: 5.3483 || 10iter: 1.9348 sec.\n",
      "Iter 6010 || Loss: 5.1394 || 10iter: 1.9322 sec.\n",
      "Iter 6020 || Loss: 5.3814 || 10iter: 1.9413 sec.\n",
      "Iter 6030 || Loss: 5.2683 || 10iter: 1.9003 sec.\n",
      "Iter 6040 || Loss: 5.4217 || 10iter: 1.9310 sec.\n",
      "Iter 6050 || Loss: 5.8575 || 10iter: 1.9128 sec.\n",
      "Iter 6060 || Loss: 5.6449 || 10iter: 1.9036 sec.\n",
      "Iter 6070 || Loss: 5.8926 || 10iter: 1.9544 sec.\n",
      "Iter 6080 || Loss: 5.6739 || 10iter: 1.8953 sec.\n",
      "Iter 6090 || Loss: 5.3722 || 10iter: 1.8978 sec.\n",
      "Iter 6100 || Loss: 4.5837 || 10iter: 1.9282 sec.\n",
      "Iter 6110 || Loss: 5.6616 || 10iter: 1.9304 sec.\n",
      "Iter 6120 || Loss: 4.8514 || 10iter: 1.9411 sec.\n",
      "Iter 6130 || Loss: 6.0938 || 10iter: 1.9359 sec.\n",
      "Iter 6140 || Loss: 4.9848 || 10iter: 1.9362 sec.\n",
      "Iter 6150 || Loss: 6.2113 || 10iter: 1.9316 sec.\n",
      "Iter 6160 || Loss: 5.5231 || 10iter: 1.9122 sec.\n",
      "Iter 6170 || Loss: 5.6310 || 10iter: 1.9237 sec.\n",
      "Iter 6180 || Loss: 4.9772 || 10iter: 1.9184 sec.\n",
      "Iter 6190 || Loss: 5.9068 || 10iter: 1.9219 sec.\n",
      "Iter 6200 || Loss: 5.9587 || 10iter: 1.8712 sec.\n",
      "Iter 6210 || Loss: 6.1575 || 10iter: 1.7432 sec.\n",
      "-------------\n",
      "epoch 6 || Epoch_TRAIN_Loss:5678.6895 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  203.5752 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 7/200\n",
      "-------------\n",
      "(train)\n",
      "Iter 6220 || Loss: 5.3276 || 10iter: 3.6976 sec.\n",
      "Iter 6230 || Loss: 5.9927 || 10iter: 1.8670 sec.\n",
      "Iter 6240 || Loss: 5.3716 || 10iter: 1.9228 sec.\n",
      "Iter 6250 || Loss: 5.0341 || 10iter: 1.8773 sec.\n",
      "Iter 6260 || Loss: 4.8785 || 10iter: 1.9555 sec.\n",
      "Iter 6270 || Loss: 5.1969 || 10iter: 1.9145 sec.\n",
      "Iter 6280 || Loss: 5.5633 || 10iter: 1.9388 sec.\n",
      "Iter 6290 || Loss: 5.0515 || 10iter: 2.0314 sec.\n",
      "Iter 6300 || Loss: 6.0010 || 10iter: 1.9420 sec.\n",
      "Iter 6310 || Loss: 5.4549 || 10iter: 1.8755 sec.\n",
      "Iter 6320 || Loss: 5.0633 || 10iter: 1.8809 sec.\n",
      "Iter 6330 || Loss: 5.2250 || 10iter: 1.9092 sec.\n",
      "Iter 6340 || Loss: 5.6289 || 10iter: 1.9018 sec.\n",
      "Iter 6350 || Loss: 5.7008 || 10iter: 1.9331 sec.\n",
      "Iter 6360 || Loss: 5.1054 || 10iter: 1.9022 sec.\n",
      "Iter 6370 || Loss: 5.1999 || 10iter: 1.8995 sec.\n",
      "Iter 6380 || Loss: 5.8094 || 10iter: 1.9219 sec.\n",
      "Iter 6390 || Loss: 5.2512 || 10iter: 1.9247 sec.\n",
      "Iter 6400 || Loss: 5.7110 || 10iter: 1.8957 sec.\n",
      "Iter 6410 || Loss: 5.1866 || 10iter: 1.9235 sec.\n",
      "Iter 6420 || Loss: 6.3414 || 10iter: 1.9359 sec.\n",
      "Iter 6430 || Loss: 5.3007 || 10iter: 1.8936 sec.\n",
      "Iter 6440 || Loss: 4.9592 || 10iter: 1.8694 sec.\n",
      "Iter 6450 || Loss: 5.3950 || 10iter: 1.9272 sec.\n",
      "Iter 6460 || Loss: 5.0373 || 10iter: 1.9082 sec.\n",
      "Iter 6470 || Loss: 5.9786 || 10iter: 1.9228 sec.\n",
      "Iter 6480 || Loss: 5.1692 || 10iter: 1.9840 sec.\n",
      "Iter 6490 || Loss: 5.0207 || 10iter: 1.9623 sec.\n",
      "Iter 6500 || Loss: 5.4802 || 10iter: 1.9513 sec.\n",
      "Iter 6510 || Loss: 4.8867 || 10iter: 1.8955 sec.\n",
      "Iter 6520 || Loss: 5.3232 || 10iter: 1.9491 sec.\n",
      "Iter 6530 || Loss: 5.6932 || 10iter: 1.9145 sec.\n",
      "Iter 6540 || Loss: 5.0516 || 10iter: 1.9212 sec.\n",
      "Iter 6550 || Loss: 6.3195 || 10iter: 1.9464 sec.\n",
      "Iter 6560 || Loss: 5.4216 || 10iter: 1.9459 sec.\n",
      "Iter 6570 || Loss: 6.1527 || 10iter: 1.9326 sec.\n",
      "Iter 6580 || Loss: 4.9233 || 10iter: 1.9031 sec.\n",
      "Iter 6590 || Loss: 5.2672 || 10iter: 1.9241 sec.\n",
      "Iter 6600 || Loss: 5.7909 || 10iter: 1.8963 sec.\n",
      "Iter 6610 || Loss: 5.0991 || 10iter: 1.9190 sec.\n",
      "Iter 6620 || Loss: 5.9675 || 10iter: 1.9053 sec.\n",
      "Iter 6630 || Loss: 4.9585 || 10iter: 1.9257 sec.\n",
      "Iter 6640 || Loss: 5.8158 || 10iter: 1.9156 sec.\n",
      "Iter 6650 || Loss: 4.7083 || 10iter: 1.9064 sec.\n",
      "Iter 6660 || Loss: 4.8529 || 10iter: 1.8876 sec.\n",
      "Iter 6670 || Loss: 6.0777 || 10iter: 1.8866 sec.\n",
      "Iter 6680 || Loss: 5.0549 || 10iter: 1.8917 sec.\n",
      "Iter 6690 || Loss: 4.9110 || 10iter: 1.9456 sec.\n",
      "Iter 6700 || Loss: 5.4523 || 10iter: 1.8810 sec.\n",
      "Iter 6710 || Loss: 5.3366 || 10iter: 1.9179 sec.\n",
      "Iter 6720 || Loss: 5.4078 || 10iter: 1.9461 sec.\n",
      "Iter 6730 || Loss: 5.6472 || 10iter: 1.9237 sec.\n",
      "Iter 6740 || Loss: 5.5324 || 10iter: 1.9232 sec.\n",
      "Iter 6750 || Loss: 5.4342 || 10iter: 1.9088 sec.\n",
      "Iter 6760 || Loss: 5.2852 || 10iter: 1.9094 sec.\n",
      "Iter 6770 || Loss: 4.8751 || 10iter: 1.9237 sec.\n",
      "Iter 6780 || Loss: 5.8178 || 10iter: 1.9029 sec.\n",
      "Iter 6790 || Loss: 5.0503 || 10iter: 1.8887 sec.\n",
      "Iter 6800 || Loss: 5.0628 || 10iter: 1.9535 sec.\n",
      "Iter 6810 || Loss: 5.8114 || 10iter: 1.8843 sec.\n",
      "Iter 6820 || Loss: 5.5054 || 10iter: 1.9081 sec.\n",
      "Iter 6830 || Loss: 5.5460 || 10iter: 1.8877 sec.\n",
      "Iter 6840 || Loss: 5.2072 || 10iter: 1.8973 sec.\n",
      "Iter 6850 || Loss: 5.4865 || 10iter: 1.9405 sec.\n",
      "Iter 6860 || Loss: 5.4291 || 10iter: 1.9139 sec.\n",
      "Iter 6870 || Loss: 5.6141 || 10iter: 1.9551 sec.\n",
      "Iter 6880 || Loss: 5.6275 || 10iter: 1.9194 sec.\n",
      "Iter 6890 || Loss: 5.3716 || 10iter: 1.9544 sec.\n",
      "Iter 6900 || Loss: 5.0124 || 10iter: 1.9088 sec.\n",
      "Iter 6910 || Loss: 5.3573 || 10iter: 1.8962 sec.\n",
      "Iter 6920 || Loss: 5.3543 || 10iter: 1.8975 sec.\n",
      "Iter 6930 || Loss: 4.9271 || 10iter: 1.9240 sec.\n",
      "Iter 6940 || Loss: 4.9061 || 10iter: 1.8924 sec.\n",
      "Iter 6950 || Loss: 4.7066 || 10iter: 1.9618 sec.\n",
      "Iter 6960 || Loss: 5.5158 || 10iter: 1.9264 sec.\n",
      "Iter 6970 || Loss: 5.0725 || 10iter: 1.9455 sec.\n",
      "Iter 6980 || Loss: 5.2451 || 10iter: 1.9331 sec.\n",
      "Iter 6990 || Loss: 5.0387 || 10iter: 1.9163 sec.\n",
      "Iter 7000 || Loss: 4.8452 || 10iter: 1.9324 sec.\n",
      "Iter 7010 || Loss: 5.0454 || 10iter: 1.9326 sec.\n",
      "Iter 7020 || Loss: 5.4362 || 10iter: 1.9154 sec.\n",
      "Iter 7030 || Loss: 4.9035 || 10iter: 1.8707 sec.\n",
      "Iter 7040 || Loss: 5.4696 || 10iter: 1.9546 sec.\n",
      "Iter 7050 || Loss: 4.7480 || 10iter: 1.9047 sec.\n",
      "Iter 7060 || Loss: 5.3677 || 10iter: 1.9549 sec.\n",
      "Iter 7070 || Loss: 5.9861 || 10iter: 1.9150 sec.\n",
      "Iter 7080 || Loss: 5.5432 || 10iter: 1.9120 sec.\n",
      "Iter 7090 || Loss: 5.1011 || 10iter: 1.9121 sec.\n",
      "Iter 7100 || Loss: 6.3403 || 10iter: 1.9217 sec.\n",
      "Iter 7110 || Loss: 5.1173 || 10iter: 1.9612 sec.\n",
      "Iter 7120 || Loss: 5.1864 || 10iter: 1.9630 sec.\n",
      "Iter 7130 || Loss: 4.8266 || 10iter: 1.9308 sec.\n",
      "Iter 7140 || Loss: 5.2680 || 10iter: 1.9285 sec.\n",
      "Iter 7150 || Loss: 5.1111 || 10iter: 1.8982 sec.\n",
      "Iter 7160 || Loss: 5.4961 || 10iter: 1.9047 sec.\n",
      "Iter 7170 || Loss: 5.3933 || 10iter: 1.9529 sec.\n",
      "Iter 7180 || Loss: 5.0901 || 10iter: 1.9352 sec.\n",
      "Iter 7190 || Loss: 5.2337 || 10iter: 1.9372 sec.\n",
      "Iter 7200 || Loss: 5.0882 || 10iter: 1.9437 sec.\n",
      "Iter 7210 || Loss: 5.6472 || 10iter: 1.9388 sec.\n",
      "Iter 7220 || Loss: 6.3728 || 10iter: 1.8769 sec.\n",
      "Iter 7230 || Loss: 4.6770 || 10iter: 1.9422 sec.\n",
      "Iter 7240 || Loss: 4.9421 || 10iter: 1.8021 sec.\n",
      "-------------\n",
      "epoch 7 || Epoch_TRAIN_Loss:5571.7516 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  203.3650 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 8/200\n",
      "-------------\n",
      "(train)\n",
      "Iter 7250 || Loss: 5.7486 || 10iter: 2.7172 sec.\n",
      "Iter 7260 || Loss: 6.0077 || 10iter: 2.0038 sec.\n",
      "Iter 7270 || Loss: 5.9509 || 10iter: 1.9195 sec.\n",
      "Iter 7280 || Loss: 5.1916 || 10iter: 1.9556 sec.\n",
      "Iter 7290 || Loss: 4.2582 || 10iter: 1.8916 sec.\n",
      "Iter 7300 || Loss: 5.0374 || 10iter: 1.9302 sec.\n",
      "Iter 7310 || Loss: 5.2894 || 10iter: 1.8731 sec.\n",
      "Iter 7320 || Loss: 4.8249 || 10iter: 1.9153 sec.\n",
      "Iter 7330 || Loss: 5.1845 || 10iter: 1.8910 sec.\n",
      "Iter 7340 || Loss: 4.7425 || 10iter: 1.8678 sec.\n",
      "Iter 7350 || Loss: 5.2149 || 10iter: 1.9193 sec.\n",
      "Iter 7360 || Loss: 5.3914 || 10iter: 1.8853 sec.\n",
      "Iter 7370 || Loss: 5.0954 || 10iter: 1.9086 sec.\n",
      "Iter 7380 || Loss: 4.9616 || 10iter: 1.9579 sec.\n",
      "Iter 7390 || Loss: 5.0336 || 10iter: 1.9258 sec.\n",
      "Iter 7400 || Loss: 5.4775 || 10iter: 1.8878 sec.\n",
      "Iter 7410 || Loss: 5.3904 || 10iter: 1.9070 sec.\n",
      "Iter 7420 || Loss: 5.0947 || 10iter: 1.9831 sec.\n",
      "Iter 7430 || Loss: 5.4183 || 10iter: 1.9790 sec.\n",
      "Iter 7440 || Loss: 6.2816 || 10iter: 2.0177 sec.\n",
      "Iter 7450 || Loss: 5.1575 || 10iter: 2.0295 sec.\n",
      "Iter 7460 || Loss: 5.8364 || 10iter: 1.9251 sec.\n",
      "Iter 7470 || Loss: 5.1171 || 10iter: 1.9054 sec.\n",
      "Iter 7480 || Loss: 4.6392 || 10iter: 1.9062 sec.\n",
      "Iter 7490 || Loss: 5.1518 || 10iter: 1.9199 sec.\n",
      "Iter 7500 || Loss: 5.3727 || 10iter: 1.9499 sec.\n",
      "Iter 7510 || Loss: 5.2105 || 10iter: 1.9581 sec.\n",
      "Iter 7520 || Loss: 4.8140 || 10iter: 1.9574 sec.\n",
      "Iter 7530 || Loss: 5.9450 || 10iter: 1.9052 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 7540 || Loss: 5.4203 || 10iter: 1.9277 sec.\n",
      "Iter 7550 || Loss: 5.6426 || 10iter: 1.9230 sec.\n",
      "Iter 7560 || Loss: 5.5743 || 10iter: 1.9535 sec.\n",
      "Iter 7570 || Loss: 4.9691 || 10iter: 1.9032 sec.\n",
      "Iter 7580 || Loss: 5.3590 || 10iter: 1.8872 sec.\n",
      "Iter 7590 || Loss: 5.3064 || 10iter: 1.9940 sec.\n",
      "Iter 7600 || Loss: 5.4071 || 10iter: 1.9078 sec.\n",
      "Iter 7610 || Loss: 5.5328 || 10iter: 1.9019 sec.\n",
      "Iter 7620 || Loss: 4.5438 || 10iter: 1.9248 sec.\n",
      "Iter 7630 || Loss: 4.6554 || 10iter: 1.8960 sec.\n",
      "Iter 7640 || Loss: 5.7452 || 10iter: 1.8856 sec.\n",
      "Iter 7650 || Loss: 5.5052 || 10iter: 2.0103 sec.\n",
      "Iter 7660 || Loss: 4.3756 || 10iter: 1.9704 sec.\n",
      "Iter 7670 || Loss: 5.0586 || 10iter: 2.0165 sec.\n",
      "Iter 7680 || Loss: 5.7227 || 10iter: 2.0719 sec.\n",
      "Iter 7690 || Loss: 5.6114 || 10iter: 2.0187 sec.\n",
      "Iter 7700 || Loss: 4.8371 || 10iter: 1.8732 sec.\n",
      "Iter 7710 || Loss: 5.3792 || 10iter: 1.9124 sec.\n",
      "Iter 7720 || Loss: 5.2324 || 10iter: 1.9028 sec.\n",
      "Iter 7730 || Loss: 5.2070 || 10iter: 1.9346 sec.\n",
      "Iter 7740 || Loss: 5.8493 || 10iter: 2.0196 sec.\n",
      "Iter 7750 || Loss: 5.4353 || 10iter: 1.9612 sec.\n",
      "Iter 7760 || Loss: 5.5819 || 10iter: 1.9216 sec.\n",
      "Iter 7770 || Loss: 6.0865 || 10iter: 1.9445 sec.\n",
      "Iter 7780 || Loss: 4.5834 || 10iter: 1.9246 sec.\n",
      "Iter 7790 || Loss: 5.7070 || 10iter: 1.9063 sec.\n",
      "Iter 7800 || Loss: 5.9228 || 10iter: 1.9155 sec.\n",
      "Iter 7810 || Loss: 4.8464 || 10iter: 1.9417 sec.\n",
      "Iter 7820 || Loss: 4.9128 || 10iter: 1.9260 sec.\n",
      "Iter 7830 || Loss: 5.6045 || 10iter: 1.9303 sec.\n",
      "Iter 7840 || Loss: 4.9797 || 10iter: 1.9510 sec.\n",
      "Iter 7850 || Loss: 5.5633 || 10iter: 1.9208 sec.\n",
      "Iter 7860 || Loss: 4.9747 || 10iter: 1.9230 sec.\n",
      "Iter 7870 || Loss: 5.2380 || 10iter: 1.9581 sec.\n",
      "Iter 7880 || Loss: 5.4359 || 10iter: 1.9057 sec.\n",
      "Iter 7890 || Loss: 4.9200 || 10iter: 1.9079 sec.\n",
      "Iter 7900 || Loss: 5.1683 || 10iter: 1.8994 sec.\n",
      "Iter 7910 || Loss: 5.9024 || 10iter: 1.9141 sec.\n",
      "Iter 7920 || Loss: 5.3544 || 10iter: 1.9035 sec.\n",
      "Iter 7930 || Loss: 5.1927 || 10iter: 1.9399 sec.\n",
      "Iter 7940 || Loss: 4.9489 || 10iter: 1.9159 sec.\n",
      "Iter 7950 || Loss: 4.5147 || 10iter: 1.9056 sec.\n",
      "Iter 7960 || Loss: 4.7378 || 10iter: 1.9057 sec.\n",
      "Iter 7970 || Loss: 4.8943 || 10iter: 1.8957 sec.\n",
      "Iter 7980 || Loss: 4.5592 || 10iter: 1.9570 sec.\n",
      "Iter 7990 || Loss: 5.1942 || 10iter: 1.9208 sec.\n",
      "Iter 8000 || Loss: 4.5264 || 10iter: 1.9165 sec.\n",
      "Iter 8010 || Loss: 4.4643 || 10iter: 1.9514 sec.\n",
      "Iter 8020 || Loss: 5.2241 || 10iter: 1.9444 sec.\n",
      "Iter 8030 || Loss: 5.5478 || 10iter: 1.9940 sec.\n",
      "Iter 8040 || Loss: 4.9554 || 10iter: 1.9710 sec.\n",
      "Iter 8050 || Loss: 5.0335 || 10iter: 1.9586 sec.\n",
      "Iter 8060 || Loss: 5.0360 || 10iter: 1.9746 sec.\n",
      "Iter 8070 || Loss: 5.0443 || 10iter: 1.9244 sec.\n",
      "Iter 8080 || Loss: 4.8811 || 10iter: 1.9076 sec.\n",
      "Iter 8090 || Loss: 4.9864 || 10iter: 1.9287 sec.\n",
      "Iter 8100 || Loss: 5.0268 || 10iter: 1.9837 sec.\n",
      "Iter 8110 || Loss: 4.8533 || 10iter: 1.8789 sec.\n",
      "Iter 8120 || Loss: 4.6960 || 10iter: 1.8951 sec.\n",
      "Iter 8130 || Loss: 5.2413 || 10iter: 1.9460 sec.\n",
      "Iter 8140 || Loss: 4.2581 || 10iter: 1.9307 sec.\n",
      "Iter 8150 || Loss: 4.8521 || 10iter: 1.9101 sec.\n",
      "Iter 8160 || Loss: 5.7603 || 10iter: 1.9492 sec.\n",
      "Iter 8170 || Loss: 5.2972 || 10iter: 1.9435 sec.\n",
      "Iter 8180 || Loss: 5.8701 || 10iter: 1.8901 sec.\n",
      "Iter 8190 || Loss: 5.1242 || 10iter: 1.9074 sec.\n",
      "Iter 8200 || Loss: 5.3602 || 10iter: 1.9090 sec.\n",
      "Iter 8210 || Loss: 5.3887 || 10iter: 1.9065 sec.\n",
      "Iter 8220 || Loss: 4.5722 || 10iter: 1.9665 sec.\n",
      "Iter 8230 || Loss: 5.3554 || 10iter: 1.9102 sec.\n",
      "Iter 8240 || Loss: 5.6532 || 10iter: 1.9284 sec.\n",
      "Iter 8250 || Loss: 6.0951 || 10iter: 1.9134 sec.\n",
      "Iter 8260 || Loss: 5.0124 || 10iter: 1.9398 sec.\n",
      "Iter 8270 || Loss: 5.8652 || 10iter: 1.8623 sec.\n",
      "Iter 8280 || Loss: 4.6907 || 10iter: 1.7396 sec.\n",
      "-------------\n",
      "epoch 8 || Epoch_TRAIN_Loss:5503.6504 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  204.5813 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 9/200\n",
      "-------------\n",
      "(train)\n",
      "Iter 8290 || Loss: 5.3625 || 10iter: 3.8027 sec.\n",
      "Iter 8300 || Loss: 5.0152 || 10iter: 1.8864 sec.\n",
      "Iter 8310 || Loss: 4.8168 || 10iter: 1.9177 sec.\n",
      "Iter 8320 || Loss: 5.1531 || 10iter: 1.9298 sec.\n",
      "Iter 8330 || Loss: 4.9626 || 10iter: 1.9128 sec.\n",
      "Iter 8340 || Loss: 4.5365 || 10iter: 1.8878 sec.\n",
      "Iter 8350 || Loss: 5.6752 || 10iter: 1.8943 sec.\n",
      "Iter 8360 || Loss: 5.3261 || 10iter: 1.9358 sec.\n",
      "Iter 8370 || Loss: 5.9367 || 10iter: 1.9033 sec.\n",
      "Iter 8380 || Loss: 4.9739 || 10iter: 1.9126 sec.\n",
      "Iter 8390 || Loss: 5.3735 || 10iter: 1.8837 sec.\n",
      "Iter 8400 || Loss: 4.7848 || 10iter: 1.9055 sec.\n",
      "Iter 8410 || Loss: 5.3975 || 10iter: 1.9068 sec.\n",
      "Iter 8420 || Loss: 5.9356 || 10iter: 1.8961 sec.\n",
      "Iter 8430 || Loss: 5.3209 || 10iter: 1.8821 sec.\n",
      "Iter 8440 || Loss: 4.8257 || 10iter: 1.9466 sec.\n",
      "Iter 8450 || Loss: 5.5585 || 10iter: 1.9185 sec.\n",
      "Iter 8460 || Loss: 4.7140 || 10iter: 1.9316 sec.\n",
      "Iter 8470 || Loss: 5.7380 || 10iter: 1.8995 sec.\n",
      "Iter 8480 || Loss: 5.0984 || 10iter: 1.9489 sec.\n",
      "Iter 8490 || Loss: 5.5181 || 10iter: 1.9049 sec.\n",
      "Iter 8500 || Loss: 4.7479 || 10iter: 1.9184 sec.\n",
      "Iter 8510 || Loss: 5.4506 || 10iter: 1.8964 sec.\n",
      "Iter 8520 || Loss: 4.9063 || 10iter: 1.8915 sec.\n",
      "Iter 8530 || Loss: 4.9865 || 10iter: 1.9343 sec.\n",
      "Iter 8540 || Loss: 5.4177 || 10iter: 1.9502 sec.\n",
      "Iter 8550 || Loss: 5.2765 || 10iter: 1.8809 sec.\n",
      "Iter 8560 || Loss: 5.7412 || 10iter: 1.9401 sec.\n",
      "Iter 8570 || Loss: 4.7722 || 10iter: 1.8989 sec.\n",
      "Iter 8580 || Loss: 4.9497 || 10iter: 1.9543 sec.\n",
      "Iter 8590 || Loss: 5.2122 || 10iter: 1.9294 sec.\n",
      "Iter 8600 || Loss: 5.4184 || 10iter: 1.9395 sec.\n",
      "Iter 8610 || Loss: 5.0879 || 10iter: 1.9035 sec.\n",
      "Iter 8620 || Loss: 6.2657 || 10iter: 1.9152 sec.\n",
      "Iter 8630 || Loss: 4.9063 || 10iter: 1.9894 sec.\n",
      "Iter 8640 || Loss: 4.7548 || 10iter: 1.9495 sec.\n",
      "Iter 8650 || Loss: 5.0629 || 10iter: 1.9096 sec.\n",
      "Iter 8660 || Loss: 5.5581 || 10iter: 1.8905 sec.\n",
      "Iter 8670 || Loss: 5.7950 || 10iter: 1.9159 sec.\n",
      "Iter 8680 || Loss: 6.2267 || 10iter: 1.8859 sec.\n",
      "Iter 8690 || Loss: 5.6308 || 10iter: 1.9473 sec.\n",
      "Iter 8700 || Loss: 5.5183 || 10iter: 1.9767 sec.\n",
      "Iter 8710 || Loss: 5.3372 || 10iter: 1.9917 sec.\n",
      "Iter 8720 || Loss: 5.3697 || 10iter: 2.0109 sec.\n",
      "Iter 8730 || Loss: 5.4175 || 10iter: 2.0299 sec.\n",
      "Iter 8740 || Loss: 5.4704 || 10iter: 2.0782 sec.\n",
      "Iter 8750 || Loss: 5.6216 || 10iter: 1.9273 sec.\n",
      "Iter 8760 || Loss: 5.3787 || 10iter: 1.8960 sec.\n",
      "Iter 8770 || Loss: 5.4913 || 10iter: 1.9225 sec.\n",
      "Iter 8780 || Loss: 5.2316 || 10iter: 1.9473 sec.\n",
      "Iter 8790 || Loss: 5.2420 || 10iter: 1.8999 sec.\n",
      "Iter 8800 || Loss: 5.1001 || 10iter: 1.9128 sec.\n",
      "Iter 8810 || Loss: 4.9831 || 10iter: 1.8909 sec.\n",
      "Iter 8820 || Loss: 5.0436 || 10iter: 1.9328 sec.\n",
      "Iter 8830 || Loss: 4.8579 || 10iter: 1.9036 sec.\n",
      "Iter 8840 || Loss: 5.5833 || 10iter: 1.9194 sec.\n",
      "Iter 8850 || Loss: 4.9346 || 10iter: 1.9502 sec.\n",
      "Iter 8860 || Loss: 4.9270 || 10iter: 1.9145 sec.\n",
      "Iter 8870 || Loss: 5.9153 || 10iter: 1.9583 sec.\n",
      "Iter 8880 || Loss: 5.7150 || 10iter: 1.9245 sec.\n",
      "Iter 8890 || Loss: 5.5443 || 10iter: 1.8980 sec.\n",
      "Iter 8900 || Loss: 5.8026 || 10iter: 1.9310 sec.\n",
      "Iter 8910 || Loss: 5.1202 || 10iter: 1.9230 sec.\n",
      "Iter 8920 || Loss: 5.3887 || 10iter: 1.8711 sec.\n",
      "Iter 8930 || Loss: 5.9049 || 10iter: 1.9401 sec.\n",
      "Iter 8940 || Loss: 5.4065 || 10iter: 2.0016 sec.\n",
      "Iter 8950 || Loss: 5.6403 || 10iter: 1.9294 sec.\n",
      "Iter 8960 || Loss: 5.2463 || 10iter: 1.9699 sec.\n",
      "Iter 8970 || Loss: 5.9570 || 10iter: 1.9660 sec.\n",
      "Iter 8980 || Loss: 5.6819 || 10iter: 1.9002 sec.\n",
      "Iter 8990 || Loss: 5.6900 || 10iter: 1.9016 sec.\n",
      "Iter 9000 || Loss: 5.0954 || 10iter: 1.9744 sec.\n",
      "Iter 9010 || Loss: 5.2382 || 10iter: 1.9220 sec.\n",
      "Iter 9020 || Loss: 5.3536 || 10iter: 1.9449 sec.\n",
      "Iter 9030 || Loss: 5.2050 || 10iter: 1.9437 sec.\n",
      "Iter 9040 || Loss: 5.7883 || 10iter: 1.9095 sec.\n",
      "Iter 9050 || Loss: 4.9560 || 10iter: 1.8664 sec.\n",
      "Iter 9060 || Loss: 5.5300 || 10iter: 1.8751 sec.\n",
      "Iter 9070 || Loss: 5.4661 || 10iter: 1.8976 sec.\n",
      "Iter 9080 || Loss: 5.0177 || 10iter: 1.8901 sec.\n",
      "Iter 9090 || Loss: 5.7886 || 10iter: 1.9078 sec.\n",
      "Iter 9100 || Loss: 5.9473 || 10iter: 1.9171 sec.\n",
      "Iter 9110 || Loss: 5.7017 || 10iter: 1.9635 sec.\n",
      "Iter 9120 || Loss: 5.3289 || 10iter: 1.9278 sec.\n",
      "Iter 9130 || Loss: 4.7557 || 10iter: 1.9191 sec.\n",
      "Iter 9140 || Loss: 4.7393 || 10iter: 1.9058 sec.\n",
      "Iter 9150 || Loss: 5.0956 || 10iter: 1.9135 sec.\n",
      "Iter 9160 || Loss: 5.1558 || 10iter: 2.1368 sec.\n",
      "Iter 9170 || Loss: 5.5507 || 10iter: 1.9152 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 9180 || Loss: 5.2094 || 10iter: 1.8939 sec.\n",
      "Iter 9190 || Loss: 5.9387 || 10iter: 1.9174 sec.\n",
      "Iter 9200 || Loss: 4.8921 || 10iter: 1.9552 sec.\n",
      "Iter 9210 || Loss: 5.4270 || 10iter: 2.0010 sec.\n",
      "Iter 9220 || Loss: 4.9059 || 10iter: 1.8728 sec.\n",
      "Iter 9230 || Loss: 5.6045 || 10iter: 1.8955 sec.\n",
      "Iter 9240 || Loss: 5.0029 || 10iter: 1.9200 sec.\n",
      "Iter 9250 || Loss: 5.0002 || 10iter: 1.9235 sec.\n",
      "Iter 9260 || Loss: 5.1014 || 10iter: 1.9489 sec.\n",
      "Iter 9270 || Loss: 4.9806 || 10iter: 1.9019 sec.\n",
      "Iter 9280 || Loss: 6.4522 || 10iter: 1.9158 sec.\n",
      "Iter 9290 || Loss: 4.9896 || 10iter: 1.9013 sec.\n",
      "Iter 9300 || Loss: 5.3658 || 10iter: 1.9329 sec.\n",
      "Iter 9310 || Loss: 5.1430 || 10iter: 1.8043 sec.\n",
      "-------------\n",
      "epoch 9 || Epoch_TRAIN_Loss:5476.7265 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  204.0673 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 10/200\n",
      "-------------\n",
      "(train)\n",
      "Iter 9320 || Loss: 4.7028 || 10iter: 2.7259 sec.\n",
      "Iter 9330 || Loss: 5.4410 || 10iter: 1.9497 sec.\n",
      "Iter 9340 || Loss: 4.3508 || 10iter: 1.8773 sec.\n",
      "Iter 9350 || Loss: 5.1644 || 10iter: 1.9103 sec.\n",
      "Iter 9360 || Loss: 5.2258 || 10iter: 1.8987 sec.\n",
      "Iter 9370 || Loss: 5.3748 || 10iter: 1.9440 sec.\n",
      "Iter 9380 || Loss: 5.4821 || 10iter: 1.8942 sec.\n",
      "Iter 9390 || Loss: 5.1075 || 10iter: 1.9149 sec.\n",
      "Iter 9400 || Loss: 5.4030 || 10iter: 1.9310 sec.\n",
      "Iter 9410 || Loss: 5.3791 || 10iter: 1.8964 sec.\n",
      "Iter 9420 || Loss: 5.7231 || 10iter: 1.9056 sec.\n",
      "Iter 9430 || Loss: 5.6963 || 10iter: 1.9259 sec.\n",
      "Iter 9440 || Loss: 5.1640 || 10iter: 1.9196 sec.\n",
      "Iter 9450 || Loss: 6.2628 || 10iter: 1.9256 sec.\n",
      "Iter 9460 || Loss: 4.9614 || 10iter: 1.8854 sec.\n",
      "Iter 9470 || Loss: 5.7793 || 10iter: 1.8860 sec.\n",
      "Iter 9480 || Loss: 5.5716 || 10iter: 1.9562 sec.\n",
      "Iter 9490 || Loss: 5.4174 || 10iter: 1.9272 sec.\n",
      "Iter 9500 || Loss: 4.6934 || 10iter: 1.9241 sec.\n",
      "Iter 9510 || Loss: 5.0595 || 10iter: 1.9233 sec.\n",
      "Iter 9520 || Loss: 5.0582 || 10iter: 1.9650 sec.\n",
      "Iter 9530 || Loss: 5.7955 || 10iter: 1.9038 sec.\n",
      "Iter 9540 || Loss: 5.2041 || 10iter: 1.9198 sec.\n",
      "Iter 9550 || Loss: 5.5626 || 10iter: 1.8656 sec.\n",
      "Iter 9560 || Loss: 5.0851 || 10iter: 1.9419 sec.\n",
      "Iter 9570 || Loss: 4.7103 || 10iter: 1.9589 sec.\n",
      "Iter 9580 || Loss: 4.9856 || 10iter: 1.9357 sec.\n",
      "Iter 9590 || Loss: 5.2953 || 10iter: 1.9591 sec.\n",
      "Iter 9600 || Loss: 4.1966 || 10iter: 1.9259 sec.\n",
      "Iter 9610 || Loss: 4.3204 || 10iter: 1.9321 sec.\n",
      "Iter 9620 || Loss: 5.0818 || 10iter: 1.9283 sec.\n",
      "Iter 9630 || Loss: 4.7996 || 10iter: 1.9095 sec.\n",
      "Iter 9640 || Loss: 5.2652 || 10iter: 1.9129 sec.\n",
      "Iter 9650 || Loss: 4.7649 || 10iter: 1.9602 sec.\n",
      "Iter 9660 || Loss: 5.5118 || 10iter: 1.9069 sec.\n",
      "Iter 9670 || Loss: 5.5286 || 10iter: 1.9708 sec.\n",
      "Iter 9680 || Loss: 4.9230 || 10iter: 1.9152 sec.\n",
      "Iter 9690 || Loss: 5.1051 || 10iter: 1.9374 sec.\n",
      "Iter 9700 || Loss: 4.8454 || 10iter: 1.9184 sec.\n",
      "Iter 9710 || Loss: 5.3123 || 10iter: 1.9110 sec.\n",
      "Iter 9720 || Loss: 5.2429 || 10iter: 1.9261 sec.\n",
      "Iter 9730 || Loss: 5.1681 || 10iter: 1.9605 sec.\n",
      "Iter 9740 || Loss: 5.2179 || 10iter: 1.9391 sec.\n",
      "Iter 9750 || Loss: 4.7723 || 10iter: 1.9348 sec.\n",
      "Iter 9760 || Loss: 5.6249 || 10iter: 1.9151 sec.\n",
      "Iter 9770 || Loss: 4.1575 || 10iter: 1.8844 sec.\n",
      "Iter 9780 || Loss: 5.4961 || 10iter: 1.9222 sec.\n",
      "Iter 9790 || Loss: 5.4147 || 10iter: 1.9126 sec.\n",
      "Iter 9800 || Loss: 4.6546 || 10iter: 1.8747 sec.\n",
      "Iter 9810 || Loss: 4.8900 || 10iter: 1.9449 sec.\n",
      "Iter 9820 || Loss: 4.7227 || 10iter: 1.8999 sec.\n",
      "Iter 9830 || Loss: 4.9963 || 10iter: 1.9162 sec.\n",
      "Iter 9840 || Loss: 5.8973 || 10iter: 1.9929 sec.\n",
      "Iter 9850 || Loss: 6.1090 || 10iter: 1.9585 sec.\n",
      "Iter 9860 || Loss: 5.4087 || 10iter: 1.9296 sec.\n",
      "Iter 9870 || Loss: 5.4700 || 10iter: 1.8836 sec.\n",
      "Iter 9880 || Loss: 4.4542 || 10iter: 1.8907 sec.\n",
      "Iter 9890 || Loss: 5.9678 || 10iter: 1.8996 sec.\n",
      "Iter 9900 || Loss: 5.3585 || 10iter: 1.9307 sec.\n",
      "Iter 9910 || Loss: 4.8997 || 10iter: 1.8838 sec.\n",
      "Iter 9920 || Loss: 5.1049 || 10iter: 1.9326 sec.\n",
      "Iter 9930 || Loss: 5.0847 || 10iter: 1.9255 sec.\n",
      "Iter 9940 || Loss: 5.1202 || 10iter: 1.9232 sec.\n",
      "Iter 9950 || Loss: 5.3619 || 10iter: 1.9409 sec.\n",
      "Iter 9960 || Loss: 5.1533 || 10iter: 1.9413 sec.\n",
      "Iter 9970 || Loss: 5.3760 || 10iter: 1.9130 sec.\n",
      "Iter 9980 || Loss: 5.1678 || 10iter: 1.8976 sec.\n",
      "Iter 9990 || Loss: 5.4373 || 10iter: 1.8803 sec.\n",
      "Iter 10000 || Loss: 5.5037 || 10iter: 1.9289 sec.\n",
      "Iter 10010 || Loss: 5.3892 || 10iter: 1.9339 sec.\n",
      "Iter 10020 || Loss: 4.6527 || 10iter: 1.9281 sec.\n",
      "Iter 10030 || Loss: 4.7520 || 10iter: 1.8943 sec.\n",
      "Iter 10040 || Loss: 5.0529 || 10iter: 1.8898 sec.\n",
      "Iter 10050 || Loss: 5.4313 || 10iter: 1.9363 sec.\n",
      "Iter 10060 || Loss: 5.7829 || 10iter: 1.9450 sec.\n",
      "Iter 10070 || Loss: 5.6492 || 10iter: 1.9231 sec.\n",
      "Iter 10080 || Loss: 4.9842 || 10iter: 1.9129 sec.\n",
      "Iter 10090 || Loss: 4.3759 || 10iter: 1.8937 sec.\n",
      "Iter 10100 || Loss: 4.7426 || 10iter: 1.9129 sec.\n",
      "Iter 10110 || Loss: 5.2795 || 10iter: 1.8920 sec.\n",
      "Iter 10120 || Loss: 4.7472 || 10iter: 1.9368 sec.\n",
      "Iter 10130 || Loss: 5.5929 || 10iter: 1.9025 sec.\n",
      "Iter 10140 || Loss: 6.0711 || 10iter: 1.9054 sec.\n",
      "Iter 10150 || Loss: 4.7826 || 10iter: 1.9033 sec.\n",
      "Iter 10160 || Loss: 5.3375 || 10iter: 1.9155 sec.\n",
      "Iter 10170 || Loss: 6.0965 || 10iter: 1.9222 sec.\n",
      "Iter 10180 || Loss: 4.8344 || 10iter: 1.9137 sec.\n",
      "Iter 10190 || Loss: 6.2410 || 10iter: 1.8879 sec.\n",
      "Iter 10200 || Loss: 4.9539 || 10iter: 1.9397 sec.\n",
      "Iter 10210 || Loss: 5.1718 || 10iter: 1.9444 sec.\n",
      "Iter 10220 || Loss: 6.1866 || 10iter: 1.9404 sec.\n",
      "Iter 10230 || Loss: 5.8270 || 10iter: 2.0026 sec.\n",
      "Iter 10240 || Loss: 5.0525 || 10iter: 1.9925 sec.\n",
      "Iter 10250 || Loss: 4.9629 || 10iter: 2.0084 sec.\n",
      "Iter 10260 || Loss: 4.3650 || 10iter: 1.9383 sec.\n",
      "Iter 10270 || Loss: 5.9446 || 10iter: 1.8766 sec.\n",
      "Iter 10280 || Loss: 4.8732 || 10iter: 1.9404 sec.\n",
      "Iter 10290 || Loss: 5.8198 || 10iter: 1.8935 sec.\n",
      "Iter 10300 || Loss: 5.3013 || 10iter: 1.8946 sec.\n",
      "Iter 10310 || Loss: 5.9169 || 10iter: 1.9248 sec.\n",
      "Iter 10320 || Loss: 4.9155 || 10iter: 1.9570 sec.\n",
      "Iter 10330 || Loss: 4.4008 || 10iter: 1.9125 sec.\n",
      "Iter 10340 || Loss: 5.0415 || 10iter: 1.9073 sec.\n",
      "Iter 10350 || Loss: 4.6114 || 10iter: 1.7326 sec.\n",
      "-------------\n",
      "(val)\n",
      "-------------\n",
      "epoch 10 || Epoch_TRAIN_Loss:5406.8323 ||Epoch_VAL_Loss:1519.4686\n",
      "timer:  246.5118 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 11/200\n",
      "-------------\n",
      "(train)\n",
      "Iter 10360 || Loss: 5.4187 || 10iter: 3.7286 sec.\n",
      "Iter 10370 || Loss: 5.0149 || 10iter: 1.9014 sec.\n",
      "Iter 10380 || Loss: 5.4323 || 10iter: 1.9052 sec.\n",
      "Iter 10390 || Loss: 5.5029 || 10iter: 1.9177 sec.\n",
      "Iter 10400 || Loss: 4.9864 || 10iter: 1.9046 sec.\n",
      "Iter 10410 || Loss: 5.1180 || 10iter: 1.9087 sec.\n",
      "Iter 10420 || Loss: 5.6769 || 10iter: 1.9221 sec.\n",
      "Iter 10430 || Loss: 5.4431 || 10iter: 1.9562 sec.\n",
      "Iter 10440 || Loss: 4.9671 || 10iter: 1.9256 sec.\n",
      "Iter 10450 || Loss: 5.2796 || 10iter: 1.9773 sec.\n",
      "Iter 10460 || Loss: 4.1618 || 10iter: 1.9207 sec.\n",
      "Iter 10470 || Loss: 5.3567 || 10iter: 1.9164 sec.\n",
      "Iter 10480 || Loss: 5.0847 || 10iter: 1.9085 sec.\n",
      "Iter 10490 || Loss: 5.0676 || 10iter: 1.8860 sec.\n",
      "Iter 10500 || Loss: 5.1934 || 10iter: 1.8841 sec.\n",
      "Iter 10510 || Loss: 4.8118 || 10iter: 1.8974 sec.\n",
      "Iter 10520 || Loss: 5.3521 || 10iter: 1.9189 sec.\n",
      "Iter 10530 || Loss: 5.2803 || 10iter: 1.9148 sec.\n",
      "Iter 10540 || Loss: 5.0059 || 10iter: 1.9575 sec.\n",
      "Iter 10550 || Loss: 5.0449 || 10iter: 2.0083 sec.\n",
      "Iter 10560 || Loss: 5.2641 || 10iter: 1.9689 sec.\n",
      "Iter 10570 || Loss: 5.2289 || 10iter: 1.9147 sec.\n",
      "Iter 10580 || Loss: 4.6226 || 10iter: 1.9335 sec.\n",
      "Iter 10590 || Loss: 5.4674 || 10iter: 1.9716 sec.\n",
      "Iter 10600 || Loss: 4.9209 || 10iter: 1.9779 sec.\n",
      "Iter 10610 || Loss: 5.2473 || 10iter: 1.9933 sec.\n",
      "Iter 10620 || Loss: 4.9255 || 10iter: 2.0157 sec.\n",
      "Iter 10630 || Loss: 5.9055 || 10iter: 1.9298 sec.\n",
      "Iter 10640 || Loss: 4.8934 || 10iter: 1.9595 sec.\n",
      "Iter 10650 || Loss: 4.8569 || 10iter: 1.9006 sec.\n",
      "Iter 10660 || Loss: 4.8509 || 10iter: 1.8770 sec.\n",
      "Iter 10670 || Loss: 4.4943 || 10iter: 1.9015 sec.\n",
      "Iter 10680 || Loss: 5.6717 || 10iter: 1.9218 sec.\n",
      "Iter 10690 || Loss: 4.9989 || 10iter: 1.9242 sec.\n",
      "Iter 10700 || Loss: 5.0211 || 10iter: 1.9570 sec.\n",
      "Iter 10710 || Loss: 4.9693 || 10iter: 1.8873 sec.\n",
      "Iter 10720 || Loss: 4.7547 || 10iter: 1.9414 sec.\n",
      "Iter 10730 || Loss: 5.1898 || 10iter: 1.9379 sec.\n",
      "Iter 10740 || Loss: 4.6290 || 10iter: 1.9163 sec.\n",
      "Iter 10750 || Loss: 5.0557 || 10iter: 1.9205 sec.\n",
      "Iter 10760 || Loss: 5.4432 || 10iter: 1.9438 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 10770 || Loss: 5.3237 || 10iter: 1.9322 sec.\n",
      "Iter 10780 || Loss: 5.6913 || 10iter: 1.9236 sec.\n",
      "Iter 10790 || Loss: 5.4656 || 10iter: 1.9038 sec.\n",
      "Iter 10800 || Loss: 5.8244 || 10iter: 1.9645 sec.\n",
      "Iter 10810 || Loss: 5.4652 || 10iter: 1.9265 sec.\n",
      "Iter 10820 || Loss: 5.6042 || 10iter: 1.8911 sec.\n",
      "Iter 10830 || Loss: 4.7095 || 10iter: 1.9338 sec.\n",
      "Iter 10840 || Loss: 5.3835 || 10iter: 1.9390 sec.\n",
      "Iter 10850 || Loss: 4.8674 || 10iter: 1.9307 sec.\n",
      "Iter 10860 || Loss: 4.8937 || 10iter: 1.9208 sec.\n",
      "Iter 10870 || Loss: 5.0901 || 10iter: 1.9088 sec.\n",
      "Iter 10880 || Loss: 5.0186 || 10iter: 1.9463 sec.\n",
      "Iter 10890 || Loss: 4.7500 || 10iter: 1.9539 sec.\n",
      "Iter 10900 || Loss: 5.4548 || 10iter: 1.9261 sec.\n",
      "Iter 10910 || Loss: 4.4846 || 10iter: 1.9562 sec.\n",
      "Iter 10920 || Loss: 5.3497 || 10iter: 1.9009 sec.\n",
      "Iter 10930 || Loss: 5.1473 || 10iter: 1.8944 sec.\n",
      "Iter 10940 || Loss: 5.4784 || 10iter: 1.9549 sec.\n",
      "Iter 10950 || Loss: 5.2840 || 10iter: 1.9256 sec.\n",
      "Iter 10960 || Loss: 4.5664 || 10iter: 1.9380 sec.\n",
      "Iter 10970 || Loss: 4.7914 || 10iter: 1.9279 sec.\n",
      "Iter 10980 || Loss: 5.0464 || 10iter: 1.9278 sec.\n",
      "Iter 10990 || Loss: 4.8586 || 10iter: 1.9597 sec.\n",
      "Iter 11000 || Loss: 5.4742 || 10iter: 2.0104 sec.\n",
      "Iter 11010 || Loss: 5.4618 || 10iter: 2.0265 sec.\n",
      "Iter 11020 || Loss: 4.9040 || 10iter: 2.0371 sec.\n",
      "Iter 11030 || Loss: 4.7851 || 10iter: 1.9326 sec.\n",
      "Iter 11040 || Loss: 5.1874 || 10iter: 1.9177 sec.\n",
      "Iter 11050 || Loss: 5.1790 || 10iter: 1.9333 sec.\n",
      "Iter 11060 || Loss: 5.3363 || 10iter: 1.9155 sec.\n",
      "Iter 11070 || Loss: 4.8506 || 10iter: 1.9328 sec.\n",
      "Iter 11080 || Loss: 4.9219 || 10iter: 1.9123 sec.\n",
      "Iter 11090 || Loss: 4.7631 || 10iter: 1.9077 sec.\n",
      "Iter 11100 || Loss: 5.0066 || 10iter: 1.9680 sec.\n",
      "Iter 11110 || Loss: 5.0048 || 10iter: 1.9300 sec.\n",
      "Iter 11120 || Loss: 4.8208 || 10iter: 1.9256 sec.\n",
      "Iter 11130 || Loss: 4.4022 || 10iter: 1.9171 sec.\n",
      "Iter 11140 || Loss: 4.9707 || 10iter: 1.9492 sec.\n",
      "Iter 11150 || Loss: 5.6035 || 10iter: 1.9628 sec.\n",
      "Iter 11160 || Loss: 5.4060 || 10iter: 1.9106 sec.\n",
      "Iter 11170 || Loss: 5.5603 || 10iter: 1.9156 sec.\n",
      "Iter 11180 || Loss: 5.1362 || 10iter: 1.8818 sec.\n",
      "Iter 11190 || Loss: 5.0182 || 10iter: 1.9177 sec.\n",
      "Iter 11200 || Loss: 5.2954 || 10iter: 1.9387 sec.\n",
      "Iter 11210 || Loss: 5.8631 || 10iter: 1.9486 sec.\n",
      "Iter 11220 || Loss: 4.9044 || 10iter: 1.9193 sec.\n",
      "Iter 11230 || Loss: 4.7383 || 10iter: 1.9050 sec.\n",
      "Iter 11240 || Loss: 5.1080 || 10iter: 1.9415 sec.\n",
      "Iter 11250 || Loss: 5.0176 || 10iter: 1.9356 sec.\n",
      "Iter 11260 || Loss: 4.7281 || 10iter: 1.9036 sec.\n",
      "Iter 11270 || Loss: 6.6153 || 10iter: 1.9473 sec.\n",
      "Iter 11280 || Loss: 5.5868 || 10iter: 1.9594 sec.\n",
      "Iter 11290 || Loss: 5.5104 || 10iter: 1.9231 sec.\n",
      "Iter 11300 || Loss: 4.9528 || 10iter: 1.9140 sec.\n",
      "Iter 11310 || Loss: 4.9470 || 10iter: 1.9025 sec.\n",
      "Iter 11320 || Loss: 5.0940 || 10iter: 1.8850 sec.\n",
      "Iter 11330 || Loss: 5.5952 || 10iter: 1.9191 sec.\n",
      "Iter 11340 || Loss: 5.4968 || 10iter: 1.8884 sec.\n",
      "Iter 11350 || Loss: 4.9171 || 10iter: 1.9275 sec.\n",
      "Iter 11360 || Loss: 5.1741 || 10iter: 1.9124 sec.\n",
      "Iter 11370 || Loss: 5.0552 || 10iter: 1.9286 sec.\n",
      "Iter 11380 || Loss: 5.3198 || 10iter: 1.8371 sec.\n",
      "-------------\n",
      "epoch 11 || Epoch_TRAIN_Loss:5344.6938 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  204.5119 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 12/200\n",
      "-------------\n",
      "(train)\n",
      "Iter 11390 || Loss: 4.7750 || 10iter: 3.0098 sec.\n",
      "Iter 11400 || Loss: 4.8311 || 10iter: 2.0108 sec.\n",
      "Iter 11410 || Loss: 5.3834 || 10iter: 1.9322 sec.\n",
      "Iter 11420 || Loss: 5.5810 || 10iter: 1.9297 sec.\n",
      "Iter 11430 || Loss: 5.1715 || 10iter: 1.9153 sec.\n",
      "Iter 11440 || Loss: 4.5261 || 10iter: 1.9059 sec.\n",
      "Iter 11450 || Loss: 5.4182 || 10iter: 1.9161 sec.\n",
      "Iter 11460 || Loss: 4.7574 || 10iter: 1.9953 sec.\n",
      "Iter 11470 || Loss: 4.8330 || 10iter: 1.9152 sec.\n",
      "Iter 11480 || Loss: 5.4270 || 10iter: 1.8767 sec.\n",
      "Iter 11490 || Loss: 4.2749 || 10iter: 1.9411 sec.\n",
      "Iter 11500 || Loss: 5.3731 || 10iter: 1.9138 sec.\n",
      "Iter 11510 || Loss: 4.7400 || 10iter: 1.9435 sec.\n",
      "Iter 11520 || Loss: 5.6541 || 10iter: 1.8850 sec.\n",
      "Iter 11530 || Loss: 5.3755 || 10iter: 1.9359 sec.\n",
      "Iter 11540 || Loss: 5.1963 || 10iter: 1.9131 sec.\n",
      "Iter 11550 || Loss: 4.9724 || 10iter: 1.9296 sec.\n",
      "Iter 11560 || Loss: 4.6669 || 10iter: 1.9225 sec.\n",
      "Iter 11570 || Loss: 4.5498 || 10iter: 1.9462 sec.\n",
      "Iter 11580 || Loss: 5.9594 || 10iter: 1.9228 sec.\n",
      "Iter 11590 || Loss: 4.3686 || 10iter: 1.9198 sec.\n",
      "Iter 11600 || Loss: 5.7748 || 10iter: 1.9111 sec.\n",
      "Iter 11610 || Loss: 5.4146 || 10iter: 1.9216 sec.\n",
      "Iter 11620 || Loss: 4.8277 || 10iter: 1.9221 sec.\n",
      "Iter 11630 || Loss: 5.7600 || 10iter: 1.9104 sec.\n",
      "Iter 11640 || Loss: 5.0438 || 10iter: 1.9202 sec.\n",
      "Iter 11650 || Loss: 5.0459 || 10iter: 1.9479 sec.\n",
      "Iter 11660 || Loss: 5.1793 || 10iter: 1.9397 sec.\n",
      "Iter 11670 || Loss: 5.1987 || 10iter: 1.9597 sec.\n",
      "Iter 11680 || Loss: 4.2431 || 10iter: 1.9195 sec.\n",
      "Iter 11690 || Loss: 5.9989 || 10iter: 1.9323 sec.\n",
      "Iter 11700 || Loss: 5.0867 || 10iter: 1.8725 sec.\n",
      "Iter 11710 || Loss: 4.4389 || 10iter: 1.9450 sec.\n",
      "Iter 11720 || Loss: 4.8250 || 10iter: 1.9484 sec.\n",
      "Iter 11730 || Loss: 5.1996 || 10iter: 1.9209 sec.\n",
      "Iter 11740 || Loss: 5.2472 || 10iter: 1.9066 sec.\n",
      "Iter 11750 || Loss: 4.3301 || 10iter: 1.9058 sec.\n",
      "Iter 11760 || Loss: 5.5556 || 10iter: 1.9063 sec.\n",
      "Iter 11770 || Loss: 5.7064 || 10iter: 1.8996 sec.\n",
      "Iter 11780 || Loss: 5.0104 || 10iter: 1.9183 sec.\n",
      "Iter 11790 || Loss: 4.8303 || 10iter: 1.9166 sec.\n",
      "Iter 11800 || Loss: 5.1812 || 10iter: 1.9357 sec.\n",
      "Iter 11810 || Loss: 5.3757 || 10iter: 1.9032 sec.\n",
      "Iter 11820 || Loss: 5.1290 || 10iter: 1.9491 sec.\n",
      "Iter 11830 || Loss: 5.5458 || 10iter: 1.9147 sec.\n",
      "Iter 11840 || Loss: 4.7665 || 10iter: 1.9126 sec.\n",
      "Iter 11850 || Loss: 4.1972 || 10iter: 1.9072 sec.\n",
      "Iter 11860 || Loss: 5.3758 || 10iter: 1.8922 sec.\n",
      "Iter 11870 || Loss: 5.2149 || 10iter: 1.8934 sec.\n",
      "Iter 11880 || Loss: 5.8153 || 10iter: 1.9030 sec.\n",
      "Iter 11890 || Loss: 5.0704 || 10iter: 1.9385 sec.\n",
      "Iter 11900 || Loss: 5.3109 || 10iter: 1.9570 sec.\n",
      "Iter 11910 || Loss: 5.0736 || 10iter: 1.9586 sec.\n",
      "Iter 11920 || Loss: 5.2753 || 10iter: 1.9265 sec.\n",
      "Iter 11930 || Loss: 5.0960 || 10iter: 1.9017 sec.\n",
      "Iter 11940 || Loss: 5.0338 || 10iter: 1.8801 sec.\n",
      "Iter 11950 || Loss: 5.4490 || 10iter: 1.9291 sec.\n",
      "Iter 11960 || Loss: 4.1153 || 10iter: 1.9147 sec.\n",
      "Iter 11970 || Loss: 4.5397 || 10iter: 1.9123 sec.\n",
      "Iter 11980 || Loss: 4.8204 || 10iter: 1.9154 sec.\n",
      "Iter 11990 || Loss: 5.1229 || 10iter: 1.9676 sec.\n",
      "Iter 12000 || Loss: 4.6447 || 10iter: 1.9459 sec.\n",
      "Iter 12010 || Loss: 5.6929 || 10iter: 1.9278 sec.\n",
      "Iter 12020 || Loss: 5.6893 || 10iter: 1.9237 sec.\n",
      "Iter 12030 || Loss: 4.8732 || 10iter: 1.9119 sec.\n",
      "Iter 12040 || Loss: 5.1677 || 10iter: 1.9736 sec.\n",
      "Iter 12050 || Loss: 4.8761 || 10iter: 1.9572 sec.\n",
      "Iter 12060 || Loss: 5.2687 || 10iter: 1.9500 sec.\n",
      "Iter 12070 || Loss: 5.3635 || 10iter: 1.9120 sec.\n",
      "Iter 12080 || Loss: 5.4838 || 10iter: 1.9132 sec.\n",
      "Iter 12090 || Loss: 5.2798 || 10iter: 1.9313 sec.\n",
      "Iter 12100 || Loss: 5.0127 || 10iter: 1.8947 sec.\n",
      "Iter 12110 || Loss: 4.7587 || 10iter: 1.8684 sec.\n",
      "Iter 12120 || Loss: 4.9004 || 10iter: 1.8883 sec.\n",
      "Iter 12130 || Loss: 4.9088 || 10iter: 1.9433 sec.\n",
      "Iter 12140 || Loss: 5.6535 || 10iter: 1.9837 sec.\n",
      "Iter 12150 || Loss: 4.6956 || 10iter: 1.9380 sec.\n",
      "Iter 12160 || Loss: 5.7253 || 10iter: 1.9108 sec.\n",
      "Iter 12170 || Loss: 5.0420 || 10iter: 1.9182 sec.\n",
      "Iter 12180 || Loss: 4.3406 || 10iter: 1.9114 sec.\n",
      "Iter 12190 || Loss: 5.4743 || 10iter: 1.9224 sec.\n",
      "Iter 12200 || Loss: 5.9723 || 10iter: 1.9161 sec.\n",
      "Iter 12210 || Loss: 5.2955 || 10iter: 1.9055 sec.\n",
      "Iter 12220 || Loss: 4.8915 || 10iter: 1.8855 sec.\n",
      "Iter 12230 || Loss: 5.0431 || 10iter: 1.9473 sec.\n",
      "Iter 12240 || Loss: 4.7737 || 10iter: 1.9236 sec.\n",
      "Iter 12250 || Loss: 5.4055 || 10iter: 1.8997 sec.\n",
      "Iter 12260 || Loss: 4.8885 || 10iter: 1.9283 sec.\n",
      "Iter 12270 || Loss: 5.8617 || 10iter: 1.9610 sec.\n",
      "Iter 12280 || Loss: 5.8804 || 10iter: 1.9146 sec.\n",
      "Iter 12290 || Loss: 4.9682 || 10iter: 1.9191 sec.\n",
      "Iter 12300 || Loss: 5.4565 || 10iter: 1.8959 sec.\n",
      "Iter 12310 || Loss: 4.9438 || 10iter: 1.8897 sec.\n",
      "Iter 12320 || Loss: 5.0288 || 10iter: 1.9383 sec.\n",
      "Iter 12330 || Loss: 5.4460 || 10iter: 1.9326 sec.\n",
      "Iter 12340 || Loss: 4.8275 || 10iter: 1.9392 sec.\n",
      "Iter 12350 || Loss: 5.2651 || 10iter: 1.9351 sec.\n",
      "Iter 12360 || Loss: 6.1065 || 10iter: 1.9240 sec.\n",
      "Iter 12370 || Loss: 5.1523 || 10iter: 1.9446 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 12380 || Loss: 5.7821 || 10iter: 1.9876 sec.\n",
      "Iter 12390 || Loss: 5.6177 || 10iter: 1.9172 sec.\n",
      "Iter 12400 || Loss: 4.6723 || 10iter: 1.9027 sec.\n",
      "Iter 12410 || Loss: 5.2864 || 10iter: 1.8680 sec.\n",
      "Iter 12420 || Loss: 5.6567 || 10iter: 1.7462 sec.\n",
      "-------------\n",
      "epoch 12 || Epoch_TRAIN_Loss:5323.9684 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  203.9996 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 13/200\n",
      "-------------\n",
      "(train)\n",
      "Iter 12430 || Loss: 5.2301 || 10iter: 3.8338 sec.\n",
      "Iter 12440 || Loss: 5.2184 || 10iter: 1.8786 sec.\n",
      "Iter 12450 || Loss: 5.0316 || 10iter: 1.8974 sec.\n",
      "Iter 12460 || Loss: 4.6722 || 10iter: 1.9490 sec.\n",
      "Iter 12470 || Loss: 5.5091 || 10iter: 1.9512 sec.\n",
      "Iter 12480 || Loss: 5.0956 || 10iter: 1.9118 sec.\n",
      "Iter 12490 || Loss: 6.1115 || 10iter: 1.9043 sec.\n",
      "Iter 12500 || Loss: 4.3868 || 10iter: 1.9399 sec.\n",
      "Iter 12510 || Loss: 5.1611 || 10iter: 1.9371 sec.\n",
      "Iter 12520 || Loss: 5.1821 || 10iter: 1.9531 sec.\n",
      "Iter 12530 || Loss: 4.9484 || 10iter: 1.8945 sec.\n",
      "Iter 12540 || Loss: 4.9977 || 10iter: 1.9271 sec.\n",
      "Iter 12550 || Loss: 4.9142 || 10iter: 1.8628 sec.\n",
      "Iter 12560 || Loss: 4.3997 || 10iter: 1.8815 sec.\n",
      "Iter 12570 || Loss: 5.4001 || 10iter: 1.9232 sec.\n",
      "Iter 12580 || Loss: 5.1149 || 10iter: 1.8984 sec.\n",
      "Iter 12590 || Loss: 5.1816 || 10iter: 1.8941 sec.\n",
      "Iter 12600 || Loss: 4.3929 || 10iter: 1.9418 sec.\n",
      "Iter 12610 || Loss: 5.1714 || 10iter: 1.9078 sec.\n",
      "Iter 12620 || Loss: 5.5324 || 10iter: 1.8891 sec.\n",
      "Iter 12630 || Loss: 5.0689 || 10iter: 1.9393 sec.\n",
      "Iter 12640 || Loss: 4.9728 || 10iter: 1.9461 sec.\n",
      "Iter 12650 || Loss: 4.9129 || 10iter: 1.8995 sec.\n",
      "Iter 12660 || Loss: 5.1512 || 10iter: 1.8970 sec.\n",
      "Iter 12670 || Loss: 5.5214 || 10iter: 1.9203 sec.\n",
      "Iter 12680 || Loss: 4.4159 || 10iter: 1.9222 sec.\n",
      "Iter 12690 || Loss: 4.8964 || 10iter: 1.9823 sec.\n",
      "Iter 12700 || Loss: 5.7488 || 10iter: 1.9428 sec.\n",
      "Iter 12710 || Loss: 5.0433 || 10iter: 1.9251 sec.\n",
      "Iter 12720 || Loss: 5.5926 || 10iter: 1.9433 sec.\n",
      "Iter 12730 || Loss: 4.6116 || 10iter: 1.9337 sec.\n",
      "Iter 12740 || Loss: 4.4592 || 10iter: 1.9689 sec.\n",
      "Iter 12750 || Loss: 4.6413 || 10iter: 1.9124 sec.\n",
      "Iter 12760 || Loss: 4.7018 || 10iter: 1.9268 sec.\n",
      "Iter 12770 || Loss: 4.9697 || 10iter: 1.9432 sec.\n",
      "Iter 12780 || Loss: 5.3603 || 10iter: 1.8884 sec.\n",
      "Iter 12790 || Loss: 4.9755 || 10iter: 1.9113 sec.\n",
      "Iter 12800 || Loss: 5.4964 || 10iter: 1.8915 sec.\n",
      "Iter 12810 || Loss: 5.0532 || 10iter: 1.9204 sec.\n",
      "Iter 12820 || Loss: 4.4793 || 10iter: 1.8983 sec.\n",
      "Iter 12830 || Loss: 5.5627 || 10iter: 1.8912 sec.\n",
      "Iter 12840 || Loss: 5.8465 || 10iter: 1.9371 sec.\n",
      "Iter 12850 || Loss: 4.7378 || 10iter: 1.9209 sec.\n",
      "Iter 12860 || Loss: 5.0794 || 10iter: 1.9230 sec.\n",
      "Iter 12870 || Loss: 5.0669 || 10iter: 1.9100 sec.\n",
      "Iter 12880 || Loss: 5.1008 || 10iter: 1.9086 sec.\n",
      "Iter 12890 || Loss: 4.8866 || 10iter: 1.9292 sec.\n",
      "Iter 12900 || Loss: 4.9822 || 10iter: 1.8927 sec.\n",
      "Iter 12910 || Loss: 5.3962 || 10iter: 1.9076 sec.\n",
      "Iter 12920 || Loss: 4.9047 || 10iter: 1.9157 sec.\n",
      "Iter 12930 || Loss: 5.3013 || 10iter: 1.9158 sec.\n",
      "Iter 12940 || Loss: 4.8607 || 10iter: 1.9111 sec.\n",
      "Iter 12950 || Loss: 5.0855 || 10iter: 1.9306 sec.\n",
      "Iter 12960 || Loss: 4.5123 || 10iter: 1.8773 sec.\n",
      "Iter 12970 || Loss: 5.2036 || 10iter: 1.8715 sec.\n",
      "Iter 12980 || Loss: 4.3877 || 10iter: 1.9267 sec.\n",
      "Iter 12990 || Loss: 4.9904 || 10iter: 1.9027 sec.\n",
      "Iter 13000 || Loss: 4.7755 || 10iter: 1.9159 sec.\n",
      "Iter 13010 || Loss: 4.8058 || 10iter: 1.9041 sec.\n",
      "Iter 13020 || Loss: 5.2457 || 10iter: 1.9061 sec.\n",
      "Iter 13030 || Loss: 4.6844 || 10iter: 1.8939 sec.\n",
      "Iter 13040 || Loss: 4.8132 || 10iter: 1.8844 sec.\n",
      "Iter 13050 || Loss: 5.0010 || 10iter: 1.8933 sec.\n",
      "Iter 13060 || Loss: 5.1245 || 10iter: 1.9875 sec.\n",
      "Iter 13070 || Loss: 4.5701 || 10iter: 1.9182 sec.\n",
      "Iter 13080 || Loss: 5.6142 || 10iter: 1.9013 sec.\n",
      "Iter 13090 || Loss: 3.7525 || 10iter: 1.9071 sec.\n",
      "Iter 13100 || Loss: 5.2722 || 10iter: 1.9548 sec.\n",
      "Iter 13110 || Loss: 6.1519 || 10iter: 1.9009 sec.\n",
      "Iter 13120 || Loss: 4.9032 || 10iter: 1.9004 sec.\n",
      "Iter 13130 || Loss: 4.4008 || 10iter: 1.9444 sec.\n",
      "Iter 13140 || Loss: 5.2507 || 10iter: 1.8855 sec.\n",
      "Iter 13150 || Loss: 4.8242 || 10iter: 1.9162 sec.\n",
      "Iter 13160 || Loss: 5.1528 || 10iter: 1.9256 sec.\n",
      "Iter 13170 || Loss: 5.7173 || 10iter: 1.8879 sec.\n",
      "Iter 13180 || Loss: 6.0553 || 10iter: 1.9058 sec.\n",
      "Iter 13190 || Loss: 4.8135 || 10iter: 1.8840 sec.\n",
      "Iter 13200 || Loss: 5.3619 || 10iter: 1.9079 sec.\n",
      "Iter 13210 || Loss: 4.9571 || 10iter: 1.9264 sec.\n",
      "Iter 13220 || Loss: 4.6518 || 10iter: 1.9454 sec.\n",
      "Iter 13230 || Loss: 5.4219 || 10iter: 1.8946 sec.\n",
      "Iter 13240 || Loss: 6.4885 || 10iter: 1.9716 sec.\n",
      "Iter 13250 || Loss: 5.1057 || 10iter: 1.8921 sec.\n",
      "Iter 13260 || Loss: 4.5243 || 10iter: 1.9594 sec.\n",
      "Iter 13270 || Loss: 5.2861 || 10iter: 1.9383 sec.\n",
      "Iter 13280 || Loss: 5.0121 || 10iter: 1.8985 sec.\n",
      "Iter 13290 || Loss: 4.9412 || 10iter: 1.9665 sec.\n",
      "Iter 13300 || Loss: 5.8559 || 10iter: 1.9757 sec.\n",
      "Iter 13310 || Loss: 5.3053 || 10iter: 1.9246 sec.\n",
      "Iter 13320 || Loss: 5.6057 || 10iter: 1.8850 sec.\n",
      "Iter 13330 || Loss: 5.1894 || 10iter: 1.9516 sec.\n",
      "Iter 13340 || Loss: 5.7093 || 10iter: 1.9000 sec.\n",
      "Iter 13350 || Loss: 5.6248 || 10iter: 1.9327 sec.\n",
      "Iter 13360 || Loss: 4.6371 || 10iter: 1.9053 sec.\n",
      "Iter 13370 || Loss: 5.5010 || 10iter: 1.8888 sec.\n",
      "Iter 13380 || Loss: 4.8812 || 10iter: 1.8967 sec.\n",
      "Iter 13390 || Loss: 5.2612 || 10iter: 1.9440 sec.\n",
      "Iter 13400 || Loss: 5.6705 || 10iter: 1.9615 sec.\n",
      "Iter 13410 || Loss: 4.6866 || 10iter: 1.9157 sec.\n",
      "Iter 13420 || Loss: 5.5697 || 10iter: 1.9359 sec.\n",
      "Iter 13430 || Loss: 5.5386 || 10iter: 1.9511 sec.\n",
      "Iter 13440 || Loss: 6.1821 || 10iter: 1.9140 sec.\n",
      "Iter 13450 || Loss: 5.0993 || 10iter: 1.8032 sec.\n",
      "-------------\n",
      "epoch 13 || Epoch_TRAIN_Loss:5291.5201 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  203.3208 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 14/200\n",
      "-------------\n",
      "(train)\n",
      "Iter 13460 || Loss: 4.5210 || 10iter: 2.6290 sec.\n",
      "Iter 13470 || Loss: 5.4727 || 10iter: 1.9444 sec.\n",
      "Iter 13480 || Loss: 5.0352 || 10iter: 1.8964 sec.\n",
      "Iter 13490 || Loss: 5.6213 || 10iter: 1.8886 sec.\n",
      "Iter 13500 || Loss: 4.9960 || 10iter: 1.8904 sec.\n",
      "Iter 13510 || Loss: 5.4474 || 10iter: 1.9474 sec.\n",
      "Iter 13520 || Loss: 4.3910 || 10iter: 1.9055 sec.\n",
      "Iter 13530 || Loss: 4.4784 || 10iter: 1.8923 sec.\n",
      "Iter 13540 || Loss: 5.3017 || 10iter: 1.9216 sec.\n",
      "Iter 13550 || Loss: 4.9129 || 10iter: 1.8835 sec.\n",
      "Iter 13560 || Loss: 5.2320 || 10iter: 1.9027 sec.\n",
      "Iter 13570 || Loss: 4.0755 || 10iter: 1.8992 sec.\n",
      "Iter 13580 || Loss: 5.4922 || 10iter: 1.9236 sec.\n",
      "Iter 13590 || Loss: 4.4459 || 10iter: 1.9332 sec.\n",
      "Iter 13600 || Loss: 4.3499 || 10iter: 1.8899 sec.\n",
      "Iter 13610 || Loss: 5.6240 || 10iter: 1.8911 sec.\n",
      "Iter 13620 || Loss: 4.7290 || 10iter: 1.9117 sec.\n",
      "Iter 13630 || Loss: 5.6805 || 10iter: 1.9328 sec.\n",
      "Iter 13640 || Loss: 4.9943 || 10iter: 1.9196 sec.\n",
      "Iter 13650 || Loss: 5.4547 || 10iter: 1.9546 sec.\n",
      "Iter 13660 || Loss: 4.7711 || 10iter: 1.9491 sec.\n",
      "Iter 13670 || Loss: 5.3370 || 10iter: 1.9140 sec.\n",
      "Iter 13680 || Loss: 5.1131 || 10iter: 1.9134 sec.\n",
      "Iter 13690 || Loss: 5.0109 || 10iter: 1.9250 sec.\n",
      "Iter 13700 || Loss: 4.9752 || 10iter: 1.9224 sec.\n",
      "Iter 13710 || Loss: 5.6353 || 10iter: 1.9001 sec.\n",
      "Iter 13720 || Loss: 5.0625 || 10iter: 1.9325 sec.\n",
      "Iter 13730 || Loss: 5.5450 || 10iter: 1.9251 sec.\n",
      "Iter 13740 || Loss: 5.6375 || 10iter: 1.9629 sec.\n",
      "Iter 13750 || Loss: 5.3169 || 10iter: 1.9374 sec.\n",
      "Iter 13760 || Loss: 5.4010 || 10iter: 1.9465 sec.\n",
      "Iter 13770 || Loss: 5.2134 || 10iter: 1.9510 sec.\n",
      "Iter 13780 || Loss: 4.8756 || 10iter: 1.9500 sec.\n",
      "Iter 13790 || Loss: 4.9692 || 10iter: 1.9252 sec.\n",
      "Iter 13800 || Loss: 5.4179 || 10iter: 1.9378 sec.\n",
      "Iter 13810 || Loss: 5.1914 || 10iter: 1.9168 sec.\n",
      "Iter 13820 || Loss: 4.9806 || 10iter: 1.9203 sec.\n",
      "Iter 13830 || Loss: 5.6334 || 10iter: 1.8896 sec.\n",
      "Iter 13840 || Loss: 4.4186 || 10iter: 1.9193 sec.\n",
      "Iter 13850 || Loss: 4.7319 || 10iter: 1.9135 sec.\n",
      "Iter 13860 || Loss: 4.7129 || 10iter: 1.9089 sec.\n",
      "Iter 13870 || Loss: 4.4372 || 10iter: 1.9218 sec.\n",
      "Iter 13880 || Loss: 4.5038 || 10iter: 1.9074 sec.\n",
      "Iter 13890 || Loss: 5.5476 || 10iter: 1.9621 sec.\n",
      "Iter 13900 || Loss: 5.3331 || 10iter: 1.9066 sec.\n",
      "Iter 13910 || Loss: 4.9898 || 10iter: 1.9182 sec.\n",
      "Iter 13920 || Loss: 4.8255 || 10iter: 1.8930 sec.\n",
      "Iter 13930 || Loss: 4.8232 || 10iter: 1.9245 sec.\n",
      "Iter 13940 || Loss: 4.5329 || 10iter: 1.9046 sec.\n",
      "Iter 13950 || Loss: 4.5911 || 10iter: 1.8953 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 13960 || Loss: 5.2317 || 10iter: 1.9100 sec.\n",
      "Iter 13970 || Loss: 5.4712 || 10iter: 1.9166 sec.\n",
      "Iter 13980 || Loss: 5.2736 || 10iter: 1.9241 sec.\n",
      "Iter 13990 || Loss: 5.4525 || 10iter: 1.9305 sec.\n",
      "Iter 14000 || Loss: 6.0101 || 10iter: 1.9083 sec.\n",
      "Iter 14010 || Loss: 5.4147 || 10iter: 1.9020 sec.\n",
      "Iter 14020 || Loss: 5.1907 || 10iter: 1.9243 sec.\n",
      "Iter 14030 || Loss: 5.1204 || 10iter: 1.9381 sec.\n",
      "Iter 14040 || Loss: 4.4785 || 10iter: 1.9198 sec.\n",
      "Iter 14050 || Loss: 4.7081 || 10iter: 1.9138 sec.\n",
      "Iter 14060 || Loss: 4.9156 || 10iter: 1.9636 sec.\n",
      "Iter 14070 || Loss: 4.7927 || 10iter: 1.9209 sec.\n",
      "Iter 14080 || Loss: 5.0901 || 10iter: 1.9288 sec.\n",
      "Iter 14090 || Loss: 5.2445 || 10iter: 1.9286 sec.\n",
      "Iter 14100 || Loss: 5.1526 || 10iter: 1.9376 sec.\n",
      "Iter 14110 || Loss: 5.8268 || 10iter: 1.9312 sec.\n",
      "Iter 14120 || Loss: 4.9961 || 10iter: 1.9255 sec.\n",
      "Iter 14130 || Loss: 5.0759 || 10iter: 1.9575 sec.\n",
      "Iter 14140 || Loss: 4.2812 || 10iter: 1.9595 sec.\n",
      "Iter 14150 || Loss: 5.6307 || 10iter: 1.9874 sec.\n",
      "Iter 14160 || Loss: 5.0764 || 10iter: 1.9451 sec.\n",
      "Iter 14170 || Loss: 5.4706 || 10iter: 1.9340 sec.\n",
      "Iter 14180 || Loss: 5.0539 || 10iter: 1.9384 sec.\n",
      "Iter 14190 || Loss: 4.9099 || 10iter: 1.9077 sec.\n",
      "Iter 14200 || Loss: 5.6264 || 10iter: 1.9492 sec.\n",
      "Iter 14210 || Loss: 5.3213 || 10iter: 2.0122 sec.\n",
      "Iter 14220 || Loss: 5.1273 || 10iter: 1.9023 sec.\n",
      "Iter 14230 || Loss: 5.1503 || 10iter: 1.8928 sec.\n",
      "Iter 14240 || Loss: 5.1845 || 10iter: 1.9076 sec.\n",
      "Iter 14250 || Loss: 4.9728 || 10iter: 1.9614 sec.\n",
      "Iter 14260 || Loss: 5.2138 || 10iter: 1.9493 sec.\n",
      "Iter 14270 || Loss: 5.2054 || 10iter: 1.9851 sec.\n",
      "Iter 14280 || Loss: 4.8065 || 10iter: 1.9160 sec.\n",
      "Iter 14290 || Loss: 4.6658 || 10iter: 1.8953 sec.\n",
      "Iter 14300 || Loss: 5.0286 || 10iter: 1.9217 sec.\n",
      "Iter 14310 || Loss: 5.5786 || 10iter: 1.8748 sec.\n",
      "Iter 14320 || Loss: 5.2729 || 10iter: 1.9038 sec.\n",
      "Iter 14330 || Loss: 5.8730 || 10iter: 1.9190 sec.\n",
      "Iter 14340 || Loss: 4.8313 || 10iter: 1.9179 sec.\n",
      "Iter 14350 || Loss: 5.4018 || 10iter: 1.9032 sec.\n",
      "Iter 14360 || Loss: 4.6700 || 10iter: 1.9147 sec.\n",
      "Iter 14370 || Loss: 5.5133 || 10iter: 1.8806 sec.\n",
      "Iter 14380 || Loss: 4.8291 || 10iter: 1.9056 sec.\n",
      "Iter 14390 || Loss: 4.5910 || 10iter: 1.9114 sec.\n",
      "Iter 14400 || Loss: 5.0783 || 10iter: 1.8987 sec.\n",
      "Iter 14410 || Loss: 4.3804 || 10iter: 1.8842 sec.\n",
      "Iter 14420 || Loss: 5.1877 || 10iter: 1.9600 sec.\n",
      "Iter 14430 || Loss: 5.1031 || 10iter: 1.9088 sec.\n",
      "Iter 14440 || Loss: 4.8108 || 10iter: 1.9229 sec.\n",
      "Iter 14450 || Loss: 4.6017 || 10iter: 1.9129 sec.\n",
      "Iter 14460 || Loss: 5.0334 || 10iter: 1.9134 sec.\n",
      "Iter 14470 || Loss: 4.8651 || 10iter: 1.9371 sec.\n",
      "Iter 14480 || Loss: 4.9189 || 10iter: 1.8544 sec.\n",
      "Iter 14490 || Loss: 4.5236 || 10iter: 1.7392 sec.\n",
      "-------------\n",
      "epoch 14 || Epoch_TRAIN_Loss:5254.5361 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  203.4716 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 15/200\n",
      "-------------\n",
      "(train)\n",
      "Iter 14500 || Loss: 4.2958 || 10iter: 3.6537 sec.\n",
      "Iter 14510 || Loss: 5.4762 || 10iter: 1.9064 sec.\n",
      "Iter 14520 || Loss: 4.6766 || 10iter: 1.9025 sec.\n",
      "Iter 14530 || Loss: 6.0038 || 10iter: 1.9602 sec.\n",
      "Iter 14540 || Loss: 4.9355 || 10iter: 1.9260 sec.\n",
      "Iter 14550 || Loss: 4.4211 || 10iter: 1.8979 sec.\n",
      "Iter 14560 || Loss: 4.8367 || 10iter: 1.9109 sec.\n",
      "Iter 14570 || Loss: 4.4004 || 10iter: 1.8890 sec.\n",
      "Iter 14580 || Loss: 3.7890 || 10iter: 1.9330 sec.\n",
      "Iter 14590 || Loss: 4.6837 || 10iter: 1.9198 sec.\n",
      "Iter 14600 || Loss: 5.3433 || 10iter: 1.9299 sec.\n",
      "Iter 14610 || Loss: 5.1091 || 10iter: 1.9408 sec.\n",
      "Iter 14620 || Loss: 5.2660 || 10iter: 1.8961 sec.\n",
      "Iter 14630 || Loss: 5.2327 || 10iter: 1.9279 sec.\n",
      "Iter 14640 || Loss: 4.4804 || 10iter: 1.9333 sec.\n",
      "Iter 14650 || Loss: 4.5865 || 10iter: 1.8844 sec.\n",
      "Iter 14660 || Loss: 5.3314 || 10iter: 1.9578 sec.\n",
      "Iter 14670 || Loss: 4.4987 || 10iter: 1.9213 sec.\n",
      "Iter 14680 || Loss: 6.4284 || 10iter: 1.9419 sec.\n",
      "Iter 14690 || Loss: 4.8119 || 10iter: 1.9075 sec.\n",
      "Iter 14700 || Loss: 5.1161 || 10iter: 1.9020 sec.\n",
      "Iter 14710 || Loss: 5.2958 || 10iter: 1.9227 sec.\n",
      "Iter 14720 || Loss: 4.5182 || 10iter: 1.9682 sec.\n",
      "Iter 14730 || Loss: 5.4464 || 10iter: 1.9210 sec.\n",
      "Iter 14740 || Loss: 5.2420 || 10iter: 1.9182 sec.\n",
      "Iter 14750 || Loss: 5.4516 || 10iter: 1.9241 sec.\n",
      "Iter 14760 || Loss: 4.9256 || 10iter: 1.9287 sec.\n",
      "Iter 14770 || Loss: 5.2563 || 10iter: 1.9332 sec.\n",
      "Iter 14780 || Loss: 5.2164 || 10iter: 1.9717 sec.\n",
      "Iter 14790 || Loss: 4.7350 || 10iter: 1.9272 sec.\n",
      "Iter 14800 || Loss: 5.0903 || 10iter: 1.8851 sec.\n",
      "Iter 14810 || Loss: 5.1602 || 10iter: 1.9280 sec.\n",
      "Iter 14820 || Loss: 5.8497 || 10iter: 1.9156 sec.\n",
      "Iter 14830 || Loss: 4.8598 || 10iter: 1.9032 sec.\n",
      "Iter 14840 || Loss: 5.1419 || 10iter: 1.9459 sec.\n",
      "Iter 14850 || Loss: 4.8418 || 10iter: 1.9331 sec.\n",
      "Iter 14860 || Loss: 5.1535 || 10iter: 1.9320 sec.\n",
      "Iter 14870 || Loss: 4.8725 || 10iter: 1.9019 sec.\n",
      "Iter 14880 || Loss: 4.5627 || 10iter: 1.9206 sec.\n",
      "Iter 14890 || Loss: 4.8348 || 10iter: 1.9818 sec.\n",
      "Iter 14900 || Loss: 4.7991 || 10iter: 1.8950 sec.\n",
      "Iter 14910 || Loss: 5.3418 || 10iter: 1.9406 sec.\n",
      "Iter 14920 || Loss: 5.2596 || 10iter: 1.9358 sec.\n",
      "Iter 14930 || Loss: 4.7718 || 10iter: 1.9231 sec.\n",
      "Iter 14940 || Loss: 5.1979 || 10iter: 1.9216 sec.\n",
      "Iter 14950 || Loss: 5.2286 || 10iter: 1.9045 sec.\n",
      "Iter 14960 || Loss: 5.3072 || 10iter: 1.8796 sec.\n",
      "Iter 14970 || Loss: 4.6527 || 10iter: 1.9409 sec.\n",
      "Iter 14980 || Loss: 5.4916 || 10iter: 1.9171 sec.\n",
      "Iter 14990 || Loss: 5.2987 || 10iter: 1.9225 sec.\n",
      "Iter 15000 || Loss: 5.1279 || 10iter: 1.8996 sec.\n",
      "Iter 15010 || Loss: 5.0290 || 10iter: 1.9094 sec.\n",
      "Iter 15020 || Loss: 5.1419 || 10iter: 1.9421 sec.\n",
      "Iter 15030 || Loss: 5.0679 || 10iter: 1.8805 sec.\n",
      "Iter 15040 || Loss: 4.3048 || 10iter: 1.9042 sec.\n",
      "Iter 15050 || Loss: 4.8955 || 10iter: 1.9439 sec.\n",
      "Iter 15060 || Loss: 4.4525 || 10iter: 1.8989 sec.\n",
      "Iter 15070 || Loss: 5.0268 || 10iter: 1.9108 sec.\n",
      "Iter 15080 || Loss: 5.2244 || 10iter: 1.9512 sec.\n",
      "Iter 15090 || Loss: 4.9594 || 10iter: 1.9051 sec.\n",
      "Iter 15100 || Loss: 4.3387 || 10iter: 1.9290 sec.\n",
      "Iter 15110 || Loss: 3.9875 || 10iter: 1.9812 sec.\n",
      "Iter 15120 || Loss: 4.5900 || 10iter: 1.9518 sec.\n",
      "Iter 15130 || Loss: 5.3459 || 10iter: 1.9837 sec.\n",
      "Iter 15140 || Loss: 5.1805 || 10iter: 1.9558 sec.\n",
      "Iter 15150 || Loss: 5.4880 || 10iter: 1.8802 sec.\n",
      "Iter 15160 || Loss: 5.1238 || 10iter: 1.9036 sec.\n",
      "Iter 15170 || Loss: 5.3673 || 10iter: 1.9646 sec.\n",
      "Iter 15180 || Loss: 4.9127 || 10iter: 1.9405 sec.\n",
      "Iter 15190 || Loss: 4.8398 || 10iter: 1.9170 sec.\n",
      "Iter 15200 || Loss: 4.5170 || 10iter: 1.8948 sec.\n",
      "Iter 15210 || Loss: 4.9435 || 10iter: 1.9327 sec.\n",
      "Iter 15220 || Loss: 5.9581 || 10iter: 1.9365 sec.\n",
      "Iter 15230 || Loss: 4.4164 || 10iter: 1.9041 sec.\n",
      "Iter 15240 || Loss: 5.7761 || 10iter: 1.9012 sec.\n",
      "Iter 15250 || Loss: 5.1189 || 10iter: 1.8908 sec.\n",
      "Iter 15260 || Loss: 5.6582 || 10iter: 1.9139 sec.\n",
      "Iter 15270 || Loss: 4.8810 || 10iter: 1.9107 sec.\n",
      "Iter 15280 || Loss: 4.7088 || 10iter: 1.9598 sec.\n",
      "Iter 15290 || Loss: 5.9506 || 10iter: 1.9530 sec.\n",
      "Iter 15300 || Loss: 4.7556 || 10iter: 1.9034 sec.\n",
      "Iter 15310 || Loss: 5.3522 || 10iter: 1.9082 sec.\n",
      "Iter 15320 || Loss: 4.8143 || 10iter: 1.8930 sec.\n",
      "Iter 15330 || Loss: 4.7779 || 10iter: 1.8796 sec.\n",
      "Iter 15340 || Loss: 5.3515 || 10iter: 1.9420 sec.\n",
      "Iter 15350 || Loss: 4.8562 || 10iter: 1.9202 sec.\n",
      "Iter 15360 || Loss: 4.5858 || 10iter: 1.9345 sec.\n",
      "Iter 15370 || Loss: 4.2171 || 10iter: 1.9174 sec.\n",
      "Iter 15380 || Loss: 4.0410 || 10iter: 1.8697 sec.\n",
      "Iter 15390 || Loss: 4.6586 || 10iter: 1.9074 sec.\n",
      "Iter 15400 || Loss: 4.7783 || 10iter: 1.9659 sec.\n",
      "Iter 15410 || Loss: 4.7335 || 10iter: 1.9520 sec.\n",
      "Iter 15420 || Loss: 5.4327 || 10iter: 1.9403 sec.\n",
      "Iter 15430 || Loss: 4.8151 || 10iter: 1.9240 sec.\n",
      "Iter 15440 || Loss: 5.5808 || 10iter: 1.9359 sec.\n",
      "Iter 15450 || Loss: 5.2550 || 10iter: 1.8985 sec.\n",
      "Iter 15460 || Loss: 4.3371 || 10iter: 1.9137 sec.\n",
      "Iter 15470 || Loss: 5.9032 || 10iter: 1.8851 sec.\n",
      "Iter 15480 || Loss: 4.9175 || 10iter: 1.9058 sec.\n",
      "Iter 15490 || Loss: 5.4471 || 10iter: 1.9238 sec.\n",
      "Iter 15500 || Loss: 5.1351 || 10iter: 1.9067 sec.\n",
      "Iter 15510 || Loss: 4.6564 || 10iter: 1.9125 sec.\n",
      "Iter 15520 || Loss: 4.9512 || 10iter: 1.8063 sec.\n",
      "-------------\n",
      "epoch 15 || Epoch_TRAIN_Loss:5214.7180 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  203.5020 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 16/200\n",
      "-------------\n",
      "(train)\n",
      "Iter 15530 || Loss: 4.8589 || 10iter: 2.6975 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 15540 || Loss: 5.8025 || 10iter: 1.9934 sec.\n",
      "Iter 15550 || Loss: 5.0796 || 10iter: 1.8539 sec.\n",
      "Iter 15560 || Loss: 5.5387 || 10iter: 1.9182 sec.\n",
      "Iter 15570 || Loss: 4.8442 || 10iter: 1.9398 sec.\n",
      "Iter 15580 || Loss: 4.8590 || 10iter: 1.9199 sec.\n",
      "Iter 15590 || Loss: 5.2256 || 10iter: 1.9237 sec.\n",
      "Iter 15600 || Loss: 4.4678 || 10iter: 1.8979 sec.\n",
      "Iter 15610 || Loss: 5.6006 || 10iter: 1.9012 sec.\n",
      "Iter 15620 || Loss: 5.3900 || 10iter: 1.9227 sec.\n",
      "Iter 15630 || Loss: 5.2369 || 10iter: 1.8937 sec.\n",
      "Iter 15640 || Loss: 5.2440 || 10iter: 1.8926 sec.\n",
      "Iter 15650 || Loss: 5.4751 || 10iter: 1.8861 sec.\n",
      "Iter 15660 || Loss: 5.3793 || 10iter: 1.9427 sec.\n",
      "Iter 15670 || Loss: 5.2494 || 10iter: 1.9208 sec.\n",
      "Iter 15680 || Loss: 5.7264 || 10iter: 1.9400 sec.\n",
      "Iter 15690 || Loss: 4.5855 || 10iter: 1.9523 sec.\n",
      "Iter 15700 || Loss: 5.5591 || 10iter: 1.9249 sec.\n",
      "Iter 15710 || Loss: 5.4837 || 10iter: 1.9512 sec.\n",
      "Iter 15720 || Loss: 4.9216 || 10iter: 1.9012 sec.\n",
      "Iter 15730 || Loss: 4.4263 || 10iter: 1.9071 sec.\n",
      "Iter 15740 || Loss: 5.4240 || 10iter: 1.8928 sec.\n",
      "Iter 15750 || Loss: 5.0460 || 10iter: 1.9402 sec.\n",
      "Iter 15760 || Loss: 5.2160 || 10iter: 1.8890 sec.\n",
      "Iter 15770 || Loss: 5.1441 || 10iter: 1.9188 sec.\n",
      "Iter 15780 || Loss: 5.5111 || 10iter: 1.9352 sec.\n",
      "Iter 15790 || Loss: 5.2516 || 10iter: 1.9114 sec.\n",
      "Iter 15800 || Loss: 4.8033 || 10iter: 1.9041 sec.\n",
      "Iter 15810 || Loss: 4.9286 || 10iter: 1.9282 sec.\n",
      "Iter 15820 || Loss: 4.9610 || 10iter: 1.9341 sec.\n",
      "Iter 15830 || Loss: 4.9275 || 10iter: 1.9478 sec.\n",
      "Iter 15840 || Loss: 5.0074 || 10iter: 1.9153 sec.\n",
      "Iter 15850 || Loss: 5.0793 || 10iter: 1.9203 sec.\n",
      "Iter 15860 || Loss: 4.4727 || 10iter: 1.9199 sec.\n",
      "Iter 15870 || Loss: 4.9073 || 10iter: 1.9862 sec.\n",
      "Iter 15880 || Loss: 4.9151 || 10iter: 1.9075 sec.\n",
      "Iter 15890 || Loss: 4.9718 || 10iter: 1.8930 sec.\n",
      "Iter 15900 || Loss: 5.2034 || 10iter: 1.9030 sec.\n",
      "Iter 15910 || Loss: 4.7498 || 10iter: 1.9010 sec.\n",
      "Iter 15920 || Loss: 5.5919 || 10iter: 1.9774 sec.\n",
      "Iter 15930 || Loss: 5.3474 || 10iter: 1.8928 sec.\n",
      "Iter 15940 || Loss: 5.4458 || 10iter: 1.8798 sec.\n",
      "Iter 15950 || Loss: 5.2783 || 10iter: 1.8855 sec.\n",
      "Iter 15960 || Loss: 5.1126 || 10iter: 1.9281 sec.\n",
      "Iter 15970 || Loss: 5.0929 || 10iter: 1.9443 sec.\n",
      "Iter 15980 || Loss: 5.4044 || 10iter: 1.9452 sec.\n",
      "Iter 15990 || Loss: 5.1864 || 10iter: 1.9404 sec.\n",
      "Iter 16000 || Loss: 4.6108 || 10iter: 1.8571 sec.\n",
      "Iter 16010 || Loss: 5.4976 || 10iter: 1.8860 sec.\n",
      "Iter 16020 || Loss: 5.3181 || 10iter: 1.9184 sec.\n",
      "Iter 16030 || Loss: 5.0945 || 10iter: 1.9540 sec.\n",
      "Iter 16040 || Loss: 4.9498 || 10iter: 1.9753 sec.\n",
      "Iter 16050 || Loss: 5.0888 || 10iter: 1.9905 sec.\n",
      "Iter 16060 || Loss: 5.1403 || 10iter: 1.9833 sec.\n",
      "Iter 16070 || Loss: 5.3758 || 10iter: 1.9911 sec.\n",
      "Iter 16080 || Loss: 4.4210 || 10iter: 2.0281 sec.\n",
      "Iter 16090 || Loss: 3.8988 || 10iter: 1.9181 sec.\n",
      "Iter 16100 || Loss: 5.1507 || 10iter: 1.9322 sec.\n",
      "Iter 16110 || Loss: 5.8649 || 10iter: 1.8971 sec.\n",
      "Iter 16120 || Loss: 4.6396 || 10iter: 1.8969 sec.\n",
      "Iter 16130 || Loss: 4.2593 || 10iter: 1.8912 sec.\n",
      "Iter 16140 || Loss: 4.6350 || 10iter: 1.9270 sec.\n",
      "Iter 16150 || Loss: 5.0824 || 10iter: 2.0418 sec.\n",
      "Iter 16160 || Loss: 5.3986 || 10iter: 1.9170 sec.\n",
      "Iter 16170 || Loss: 5.4058 || 10iter: 1.9110 sec.\n",
      "Iter 16180 || Loss: 5.1361 || 10iter: 1.9350 sec.\n",
      "Iter 16190 || Loss: 4.5617 || 10iter: 1.8979 sec.\n",
      "Iter 16200 || Loss: 4.7005 || 10iter: 1.9658 sec.\n",
      "Iter 16210 || Loss: 5.0582 || 10iter: 1.9073 sec.\n",
      "Iter 16220 || Loss: 5.3389 || 10iter: 1.9294 sec.\n",
      "Iter 16230 || Loss: 4.6105 || 10iter: 1.9419 sec.\n",
      "Iter 16240 || Loss: 5.7215 || 10iter: 1.9468 sec.\n",
      "Iter 16250 || Loss: 5.0735 || 10iter: 1.9289 sec.\n",
      "Iter 16260 || Loss: 4.6622 || 10iter: 1.9304 sec.\n",
      "Iter 16270 || Loss: 4.5883 || 10iter: 1.9470 sec.\n",
      "Iter 16280 || Loss: 4.7803 || 10iter: 1.9388 sec.\n",
      "Iter 16290 || Loss: 4.6865 || 10iter: 1.8859 sec.\n",
      "Iter 16300 || Loss: 4.6144 || 10iter: 1.9257 sec.\n",
      "Iter 16310 || Loss: 5.0478 || 10iter: 1.9122 sec.\n",
      "Iter 16320 || Loss: 5.5735 || 10iter: 1.9097 sec.\n",
      "Iter 16330 || Loss: 4.9333 || 10iter: 1.9100 sec.\n",
      "Iter 16340 || Loss: 4.7054 || 10iter: 1.8936 sec.\n",
      "Iter 16350 || Loss: 5.0866 || 10iter: 1.9577 sec.\n",
      "Iter 16360 || Loss: 4.8149 || 10iter: 1.9928 sec.\n",
      "Iter 16370 || Loss: 4.2935 || 10iter: 1.9782 sec.\n",
      "Iter 16380 || Loss: 4.7538 || 10iter: 2.0578 sec.\n",
      "Iter 16390 || Loss: 5.2114 || 10iter: 2.1233 sec.\n",
      "Iter 16400 || Loss: 4.7524 || 10iter: 2.0172 sec.\n",
      "Iter 16410 || Loss: 4.8717 || 10iter: 2.0131 sec.\n",
      "Iter 16420 || Loss: 4.8024 || 10iter: 1.9762 sec.\n",
      "Iter 16430 || Loss: 6.0655 || 10iter: 2.0855 sec.\n",
      "Iter 16440 || Loss: 4.5962 || 10iter: 2.0473 sec.\n",
      "Iter 16450 || Loss: 5.3304 || 10iter: 2.1144 sec.\n",
      "Iter 16460 || Loss: 5.3420 || 10iter: 2.1251 sec.\n",
      "Iter 16470 || Loss: 4.8643 || 10iter: 1.9269 sec.\n",
      "Iter 16480 || Loss: 5.1812 || 10iter: 1.9320 sec.\n",
      "Iter 16490 || Loss: 4.8109 || 10iter: 1.9089 sec.\n",
      "Iter 16500 || Loss: 5.2851 || 10iter: 1.8870 sec.\n",
      "Iter 16510 || Loss: 5.0556 || 10iter: 1.9395 sec.\n"
     ]
    }
   ],
   "source": [
    "if DATASET == \"COCO\":\n",
    "    num_epochs = 200\n",
    "else:\n",
    "    num_epochs = 200\n",
    "    \n",
    "train_model(net, dataloaders_dict, criterion, optimizer, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
